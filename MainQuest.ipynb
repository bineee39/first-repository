{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyMTqvIfs9829lK17i72WdxL",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/bineee39/first-repository/blob/master/MainQuest.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "PTMFZRDu8Fyl"
      },
      "outputs": [],
      "source": [
        "import numpy as np\n",
        "import pandas as pd\n",
        "import matplotlib.pyplot as plt\n",
        "import seaborn as sns\n",
        "from tqdm import tqdm\n",
        "\n",
        "train_df = pd.read_csv('./train.csv')\n",
        "test_df = pd.read_csv('./test.csv')"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "train_df.shape"
      ],
      "metadata": {
        "id": "9-KrmcCjGUcs"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "test_df.shape"
      ],
      "metadata": {
        "id": "92wbyIYGGYvt"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "train_df.describe()"
      ],
      "metadata": {
        "id": "zyBJvKWWGet9"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "train_df['id'].nunique()"
      ],
      "metadata": {
        "id": "tpLcdcQUJnDg"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "sns.scatterplot(train_df,x=train_df['id'],y=train_df['Time'])"
      ],
      "metadata": {
        "id": "0Rxcyd-mKvUx"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "id에따라 time이 증가한다. id를 부여할때 time을 오름차순으로 두고 부여한것으로 보인다."
      ],
      "metadata": {
        "id": "Olv8gClgc_eo"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "sns.scatterplot(train_df,x=train_df['Time'],y=train_df['Amount'])"
      ],
      "metadata": {
        "id": "NDbWjbArLIUJ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "이상치가 사기거래를 탐지하는 역할을 할 수도 있으므로 이상치를 제거하기보단 스케일링만 진행했습니다."
      ],
      "metadata": {
        "id": "Hb7HaZM2ZeYF"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "sns.countplot(x='Class', data=train_df)"
      ],
      "metadata": {
        "id": "a2TfhIClL58I"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "train_df['Class'].value_counts()"
      ],
      "metadata": {
        "id": "A93uu7xqMclD"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "train_df['Class'].value_counts(normalize=True)"
      ],
      "metadata": {
        "id": "lOrkat2tM7s3"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import seaborn as sns\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "corr_matrix = train_df.drop('id', axis=1).corr()\n",
        "\n",
        "corr_with_class = corr_matrix[['Class']]\n",
        "\n",
        "plt.figure(figsize=(15, 8))\n",
        "sns.heatmap(corr_with_class.sort_values(by='Class', ascending=False),\n",
        "            annot=True,\n",
        "            fmt='.2f',\n",
        "            cmap='coolwarm')\n",
        "plt.title('Correlation of all features with Class')\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "rcwMPbVnNDBE"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "이미 pca를 거친 변수들이므로 상관계수가 낮게나온다"
      ],
      "metadata": {
        "id": "qW7c_KARcumv"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.preprocessing import RobustScaler\n",
        "\n",
        "scaler = RobustScaler()\n",
        "cols_to_scale = ['Time', 'Amount']\n",
        "\n",
        "train_df[cols_to_scale] = scaler.fit_transform(train_df[cols_to_scale])\n",
        "\n",
        "test_df[cols_to_scale] = scaler.transform(test_df[cols_to_scale])\n",
        "\n",
        "train_df[cols_to_scale].describe()"
      ],
      "metadata": {
        "id": "6XYcZTC6ZRkH"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "train_df.head()"
      ],
      "metadata": {
        "id": "u76G-ET1ZYf9"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "train_df.drop('id',axis=1,inplace=True)"
      ],
      "metadata": {
        "id": "jnUWAB_RZ0gr"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "test_df.drop('id',axis=1,inplace=True)"
      ],
      "metadata": {
        "id": "cR89sQZzaBrS"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "test_df.head()"
      ],
      "metadata": {
        "id": "TEIOQOAQaVmv"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "X = train_df.drop('Class', axis=1)\n",
        "y = train_df['Class']\n",
        "\n",
        "print(f\"X shape: {X.shape}\")\n",
        "print(f\"y shape: {y.shape}\")"
      ],
      "metadata": {
        "id": "Cp6twqZjaXmK"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "from sklearn.model_selection import train_test_split\n",
        "from imblearn.over_sampling import SMOTE\n",
        "from sklearn.linear_model import LogisticRegression\n",
        "from lightgbm import LGBMClassifier\n",
        "from xgboost import XGBClassifier\n",
        "from sklearn.metrics import f1_score, classification_report, confusion_matrix, accuracy_score, recall_score, precision_score\n",
        "\n",
        "X_train, X_val, y_train, y_val = train_test_split(\n",
        "    X,\n",
        "    y,\n",
        "    test_size=0.2,\n",
        "    shuffle=False,\n",
        "    random_state=42\n",
        ")\n",
        "\n",
        "print(f\"원본 y_train 클래스 분포:\\n{y_train.value_counts()}\")\n",
        "#오버샘플링으로 불균형 Class맞추어줌\n",
        "smote = SMOTE(random_state=42)\n",
        "X_train_smote, y_train_smote = smote.fit_resample(X_train, y_train)\n",
        "\n",
        "print(f\"SMOTE 적용 후 y_train 클래스 분포:\\n{y_train_smote.value_counts()}\")\n",
        "\n",
        "models = {\n",
        "    \"Logistic Regression (Lasso)\": LogisticRegression(penalty='l1', C=1.0, solver='liblinear', random_state=42),\n",
        "    \"LightGBM\": LGBMClassifier(random_state=42, n_jobs=-1, verbosity=-1),\n",
        "    \"XGBoost\": XGBClassifier(random_state=42, n_jobs=-1, eval_metric='logloss')\n",
        "}\n",
        "\n",
        "results = []\n",
        "\n",
        "for name, model in models.items():\n",
        "    model.fit(X_train_smote, y_train_smote)\n",
        "    y_pred = model.predict(X_val)\n",
        "\n",
        "    accuracy = accuracy_score(y_val, y_pred)\n",
        "    precision = precision_score(y_val, y_pred)\n",
        "    recall = recall_score(y_val, y_pred)\n",
        "    f1 = f1_score(y_val, y_pred)\n",
        "\n",
        "    results.append({\n",
        "        'Model': name,\n",
        "        'Accuracy': accuracy,\n",
        "        'Precision': precision,\n",
        "        'Recall': recall,\n",
        "        'F1 Score': f1\n",
        "    })\n",
        "\n",
        "    print(f\"[{name}] 검증 데이터 F1 Score: {f1:.4f}\")\n",
        "\n",
        "    print(\"\\n[Classification Report]\")\n",
        "    print(classification_report(y_val, y_pred, target_names=['Normal (0)', 'Fraud (1)']))\n",
        "\n",
        "final_results_df = pd.DataFrame(results).set_index('Model')\n",
        "\n",
        "print(\"최종 성능 지표\")\n",
        "print(final_results_df.sort_values(by='F1 Score', ascending=False))"
      ],
      "metadata": {
        "id": "zbRd_oNMa6Sq"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "하이퍼파라미터 최적화를 거치지 않은 상태에서는 XGBoost가 가장 F1score가 높았다"
      ],
      "metadata": {
        "id": "ApYzi-ENf3hu"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install optuna"
      ],
      "metadata": {
        "id": "ebTuIjIdf_b5"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# 로지스틱 하이퍼파라미터(C) 최적화"
      ],
      "metadata": {
        "id": "P180d9TvgLpo"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import optuna\n",
        "from sklearn.linear_model import LogisticRegression\n",
        "from sklearn.metrics import f1_score\n",
        "from imblearn.over_sampling import SMOTE\n",
        "def objective_lr(trial):\n",
        "\n",
        "    c_param = trial.suggest_loguniform('C', 1e-4, 100)\n",
        "\n",
        "    model = LogisticRegression(\n",
        "        penalty='l1',\n",
        "        C=c_param,\n",
        "        solver='liblinear',\n",
        "        random_state=42\n",
        "    )\n",
        "\n",
        "    model.fit(X_train_smote, y_train_smote)\n",
        "    y_pred = model.predict(X_val)\n",
        "\n",
        "    return f1_score(y_val, y_pred)\n",
        "\n",
        "study_lr = optuna.create_study(direction='maximize')\n",
        "study_lr.optimize(objective_lr, n_trials=50, show_progress_bar=True)\n",
        "\n",
        "print(\"\\n[로지스틱 회귀] 최적 F1 Score:\", study_lr.best_value)\n",
        "print(\"[로지스틱 회귀] 최적 하이퍼파라미터:\", study_lr.best_params)\n",
        "\n",
        "best_lr_params = study_lr.best_params\n",
        "best_lr_model = LogisticRegression(penalty='l1', solver='liblinear', random_state=42, **best_lr_params)\n",
        "best_lr_model.fit(X_train_smote, y_train_smote)"
      ],
      "metadata": {
        "id": "FVwTgmUvgG79"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "F1 score가 높아지긴 했지만 0.2로 유의미한 결과를 주진 못했다."
      ],
      "metadata": {
        "id": "tvQBOVfi1Wh2"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from lightgbm import LGBMClassifier\n",
        "def objective_lgbm(trial):\n",
        "    lgbm_params = {\n",
        "        'n_estimators': trial.suggest_int('n_estimators', 500, 2000),\n",
        "        'learning_rate': trial.suggest_loguniform('learning_rate', 0.01, 0.1),\n",
        "        'max_depth': trial.suggest_int('max_depth', 5, 12),\n",
        "        'num_leaves': trial.suggest_int('num_leaves', 31, 100),\n",
        "        'min_child_samples': trial.suggest_int('min_child_samples', 20, 100),\n",
        "        'subsample': trial.suggest_uniform('subsample', 0.6, 1.0),\n",
        "        'colsample_bytree': trial.suggest_uniform('colsample_bytree', 0.6, 1.0),\n",
        "        'reg_alpha': trial.suggest_loguniform('reg_alpha', 1e-8, 10.0),\n",
        "        'reg_lambda': trial.suggest_loguniform('reg_lambda', 1e-8, 10.0),\n",
        "    }\n",
        "\n",
        "    model = LGBMClassifier(\n",
        "        random_state=42,\n",
        "        n_jobs=-1,\n",
        "        verbosity=-1,\n",
        "        **lgbm_params\n",
        "    )\n",
        "\n",
        "    model.fit(X_train_smote, y_train_smote)\n",
        "    y_pred = model.predict(X_val)\n",
        "    return f1_score(y_val, y_pred)\n",
        "study_lgbm = optuna.create_study(direction='maximize')\n",
        "study_lgbm.optimize(objective_lgbm, n_trials=100, show_progress_bar=True) # 더 많은 시도 (n_trials) 추천\n",
        "\n",
        "print(\"\\n[LightGBM] 최적 F1 Score:\", study_lgbm.best_value)\n",
        "print(\"[LightGBM] 최적 하이퍼파라미터:\", study_lgbm.best_params)\n",
        "\n",
        "\n",
        "best_lgbm_params = study_lgbm.best_params\n",
        "best_lgbm_model = LGBMClassifier(random_state=42, n_jobs=-1, verbosity=-1, **best_lgbm_params)\n",
        "best_lgbm_model.fit(X_train_smote, y_train_smote)"
      ],
      "metadata": {
        "id": "1p_gHnqUhkEH"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from xgboost import XGBClassifier\n",
        "\n",
        "def objective_xgb(trial):\n",
        "    xgb_params = {\n",
        "        'n_estimators': trial.suggest_int('n_estimators', 500, 2000),\n",
        "        'learning_rate': trial.suggest_loguniform('learning_rate', 0.01, 0.1),\n",
        "        'max_depth': trial.suggest_int('max_depth', 5, 12),\n",
        "        'min_child_weight': trial.suggest_int('min_child_weight', 1, 10),\n",
        "        'gamma': trial.suggest_loguniform('gamma', 1e-8, 1.0),\n",
        "        'subsample': trial.suggest_uniform('subsample', 0.6, 1.0),\n",
        "        'colsample_bytree': trial.suggest_uniform('colsample_bytree', 0.6, 1.0),\n",
        "        'reg_alpha': trial.suggest_loguniform('reg_alpha', 1e-8, 10.0),\n",
        "        'reg_lambda': trial.suggest_loguniform('reg_lambda', 1e-8, 10.0),\n",
        "        'eval_metric': 'logloss'\n",
        "    }\n",
        "\n",
        "    model = XGBClassifier(\n",
        "        random_state=42,\n",
        "        n_jobs=-1,\n",
        "        use_label_encoder=False,\n",
        "        **xgb_params\n",
        "    )\n",
        "\n",
        "    model.fit(X_train_smote, y_train_smote)\n",
        "    y_pred = model.predict(X_val)\n",
        "    return f1_score(y_val, y_pred)\n",
        "\n",
        "study_xgb = optuna.create_study(direction='maximize')\n",
        "study_xgb.optimize(objective_xgb, n_trials=100, show_progress_bar=True)\n",
        "\n",
        "print(\"\\n[XGBoost] 최적 F1 Score:\", study_xgb.best_value)\n",
        "print(\"[XGBoost] 최적 하이퍼파라미터:\", study_xgb.best_params)\n",
        "\n",
        "best_xgb_params = study_xgb.best_params\n",
        "best_xgb_model = XGBClassifier(random_state=42, n_jobs=-1, use_label_encoder=False, **best_xgb_params)\n",
        "best_xgb_model.fit(X_train_smote, y_train_smote)"
      ],
      "metadata": {
        "id": "i-Q6FZ-bhsyV"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from lightgbm import LGBMClassifier\n",
        "from xgboost import XGBClassifier\n",
        "from sklearn.ensemble import VotingClassifier\n",
        "\n",
        "\n",
        "best_lgbm_params = {'n_estimators': 949, 'learning_rate': 0.015103133155498064, 'max_depth': 12, 'num_leaves': 85, 'min_child_samples': 52, 'subsample': 0.6465824063100066, 'colsample_bytree': 0.8516036360089998, 'reg_alpha': 1.2742027666408652e-07, 'reg_lambda': 4.704365280380022e-08}\n",
        "best_xgb_params = {'n_estimators': 792, 'learning_rate': 0.011287998067292129, 'max_depth': 12, 'min_child_weight': 1, 'gamma': 1.0236396419890437e-06, 'subsample': 0.9579569591763855, 'colsample_bytree': 0.7647619069349275, 'reg_alpha': 0.0014023169752579944, 'reg_lambda': 2.45071729037708e-05}\n",
        "\n",
        "\n",
        "\n",
        "lgbm_clf = LGBMClassifier(random_state=42, n_jobs=-1, verbosity=-1, **best_lgbm_params)\n",
        "xgb_clf = XGBClassifier(random_state=42, n_jobs=-1, eval_metric='logloss', use_label_encoder=False, **best_xgb_params)\n",
        "\n",
        "\n",
        "lgbm_clf.fit(X_train_smote, y_train_smote)\n",
        "xgb_clf.fit(X_train_smote, y_train_smote)\n",
        "\n",
        "\n",
        "voting_clf = VotingClassifier(\n",
        "    estimators=[('lgbm', lgbm_clf), ('xgb', xgb_clf)],\n",
        "    voting='soft',\n",
        "    n_jobs=-1\n",
        ")\n",
        "\n",
        "print(\"앙상블 모델 학습 시작...\")\n",
        "voting_clf.fit(X_train_smote, y_train_smote)\n",
        "print(\"앙상블 모델 학습 완료.\")"
      ],
      "metadata": {
        "id": "hvOw8OFcIsQF"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "앙상블로 묶음 (다양한 분류모델로 진행하였으므로 voting기법으로 묶음)"
      ],
      "metadata": {
        "id": "8AJL1hepmMoF"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.metrics import f1_score\n",
        "\n",
        "y_pred_voting = voting_clf.predict(X_val)\n",
        "\n",
        "f1_voting = f1_score(y_val, y_pred_voting)\n",
        "\n",
        "print(f\"앙상블 F1 Score: {f1_voting:.4f}\")\n",
        "\n",
        "print(f\"   XGBoost : 0.8438\")\n",
        "print(f\"   LightGBM : 0.8351\")"
      ],
      "metadata": {
        "id": "ivS5-xS0l5Mj"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "\n",
        "final_model = voting_clf\n",
        "\n",
        "\n",
        "y_proba = final_model.predict_proba(X_val)[:, 1]\n",
        "\n",
        "best_f1 = 0\n",
        "best_threshold = 0.5\n",
        "\n",
        "\n",
        "for threshold in np.arange(0.05, 0.95, 0.01):\n",
        "\n",
        "    y_pred_threshold = (y_proba >= threshold).astype(int)\n",
        "\n",
        "\n",
        "    f1 = f1_score(y_val, y_pred_threshold)\n",
        "\n",
        "    if f1 > best_f1:\n",
        "        best_f1 = f1\n",
        "        best_threshold = threshold\n",
        "\n",
        "print(f\"최적 임곗값: {best_threshold:.2f}\")\n",
        "print(f\"최종 F1 Score: {best_f1:.4f}\")"
      ],
      "metadata": {
        "id": "hmOtjTuhmuqy"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "import numpy as np\n",
        "\n",
        "original_test_df = pd.read_csv('./test.csv')\n",
        "test_ids_for_submission = original_test_df['id']\n",
        "\n",
        "cols_to_scale = ['Time', 'Amount']\n",
        "\n",
        "original_test_df[cols_to_scale] = scaler.transform(original_test_df[cols_to_scale])\n",
        "\n",
        "X_test_final = original_test_df.drop('id', axis=1)\n",
        "\n",
        "final_model = voting_clf\n",
        "best_threshold = 0.59\n",
        "\n",
        "\n",
        "test_proba = final_model.predict_proba(X_test_final)[:, 1]\n",
        "\n",
        "\n",
        "final_predictions = (test_proba >= best_threshold).astype(int)\n",
        "\n",
        "\n",
        "submission = pd.DataFrame({\n",
        "    \"id\": test_ids_for_submission,\n",
        "    \"Class\": final_predictions\n",
        "})\n",
        "\n",
        "\n",
        "submission.to_csv(\"./submission_gyubin.csv\", index=False)\n",
        "\n",
        "print(\"예측 개수는\", len(submission), \"건입니다.\")"
      ],
      "metadata": {
        "id": "-tSVlFI4nqkE"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "과적합된것같아서 좀 더 느슨하게 다시 최적의 파라미터를 찾음"
      ],
      "metadata": {
        "id": "rgpGeBfhE0Y3"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import optuna\n",
        "from xgboost import XGBClassifier\n",
        "from sklearn.metrics import f1_score\n",
        "\n",
        "\n",
        "def objective_xgb_re(trial):\n",
        "    xgb_params = {\n",
        "        'n_estimators': trial.suggest_int('n_estimators', 500, 2000),\n",
        "        'learning_rate': trial.suggest_loguniform('learning_rate', 0.01, 0.1),\n",
        "        'max_depth': trial.suggest_int('max_depth', 5, 9),\n",
        "        'min_child_weight': trial.suggest_int('min_child_weight', 1, 10),\n",
        "        'gamma': trial.suggest_loguniform('gamma', 1e-8, 1.0),\n",
        "        'subsample': trial.suggest_uniform('subsample', 0.6, 1.0),\n",
        "        'colsample_bytree': trial.suggest_uniform('colsample_bytree', 0.6, 1.0),\n",
        "        'reg_alpha': trial.suggest_loguniform('reg_alpha', 1e-5, 10.0),\n",
        "        'reg_lambda': trial.suggest_loguniform('reg_lambda', 1e-5, 10.0),\n",
        "        'eval_metric': 'logloss'\n",
        "    }\n",
        "\n",
        "    model = XGBClassifier(\n",
        "        random_state=42,\n",
        "        n_jobs=-1,\n",
        "        use_label_encoder=False,\n",
        "        **xgb_params\n",
        "    )\n",
        "\n",
        "    model.fit(X_train_smote, y_train_smote)\n",
        "    y_pred = model.predict(X_val)\n",
        "    return f1_score(y_val, y_pred)\n",
        "\n",
        "study_xgb_re = optuna.create_study(direction='maximize')\n",
        "study_xgb_re.optimize(objective_xgb_re, n_trials=50, show_progress_bar=True)\n",
        "\n",
        "print(\"\\n[ XGBoost] 최적 F1 Score:\", study_xgb_re.best_value)\n",
        "print(\"[ XGBoost] 최적 하이퍼파라미터:\", study_xgb_re.best_params)\n",
        "best_xgb_params_re = study_xgb_re.best_params"
      ],
      "metadata": {
        "id": "YgZf8ZA75KUE"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import optuna\n",
        "from lightgbm import LGBMClassifier\n",
        "from sklearn.metrics import f1_score\n",
        "\n",
        "def objective_lgbm_re(trial):\n",
        "    lgbm_params = {\n",
        "        'n_estimators': trial.suggest_int('n_estimators', 500, 2000),\n",
        "        'learning_rate': trial.suggest_loguniform('learning_rate', 0.01, 0.1),\n",
        "        'max_depth': trial.suggest_int('max_depth', 5, 9),\n",
        "        'num_leaves': trial.suggest_int('num_leaves', 31, 70),\n",
        "        'min_child_samples': trial.suggest_int('min_child_samples', 20, 100),\n",
        "        'subsample': trial.suggest_uniform('subsample', 0.6, 1.0),\n",
        "        'colsample_bytree': trial.suggest_uniform('colsample_bytree', 0.6, 1.0),\n",
        "        'reg_alpha': trial.suggest_loguniform('reg_alpha', 1e-5, 10.0),\n",
        "        'reg_lambda': trial.suggest_loguniform('reg_lambda', 1e-5, 10.0),\n",
        "    }\n",
        "\n",
        "    model = LGBMClassifier(\n",
        "        random_state=42,\n",
        "        n_jobs=-1,\n",
        "        verbosity=-1,\n",
        "        **lgbm_params\n",
        "    )\n",
        "\n",
        "    model.fit(X_train_smote, y_train_smote)\n",
        "    y_pred = model.predict(X_val)\n",
        "    return f1_score(y_val, y_pred)\n",
        "\n",
        "print(\"---  LightGBM 최적화 시작 (n_trials=50) ---\")\n",
        "study_lgbm_re = optuna.create_study(direction='maximize')\n",
        "study_lgbm_re.optimize(objective_lgbm_re, n_trials=50, show_progress_bar=True)\n",
        "\n",
        "print(\"\\n LightGBM] 최적 F1 Score:\", study_lgbm_re.best_value)\n",
        "print(\"LightGBM] 최적 하이퍼파라미터:\", study_lgbm_re.best_params)\n",
        "\n",
        "best_lgbm_params_re = study_lgbm_re.best_params"
      ],
      "metadata": {
        "id": "SyrsSesl2qII"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import optuna\n",
        "from sklearn.ensemble import VotingClassifier\n",
        "from lightgbm import LGBMClassifier\n",
        "from xgboost import XGBClassifier\n",
        "from sklearn.metrics import f1_score\n",
        "import numpy as np\n",
        "\n",
        "best_lgbm_params = {'n_estimators': 874, 'learning_rate': 0.08326062054708025, 'max_depth': 8, 'num_leaves': 62, 'min_child_samples': 71, 'subsample': 0.9737480587317234, 'colsample_bytree': 0.8917908434852498, 'reg_alpha': 0.013213503671865254, 'reg_lambda': 5.4226824776857555e-05}\n",
        "best_xgb_params = {'n_estimators': 921, 'learning_rate': 0.023703360450466202, 'max_depth': 9, 'min_child_weight': 1, 'gamma': 0.38887847738437764, 'subsample': 0.8954154427644301, 'colsample_bytree': 0.6157837367341784, 'reg_alpha': 0.005934695765543821, 'reg_lambda': 0.0010594014155312593}\n",
        "\n",
        "\n",
        "def objective_voting_weights_re(trial):\n",
        "    w1_lgbm = trial.suggest_float('w1_lgbm', 0.5, 1.0)\n",
        "    w2_xgb = 1.0 - w1_lgbm\n",
        "\n",
        "\n",
        "    lgbm_clf_opt = LGBMClassifier(random_state=42, n_jobs=-1, verbosity=-1, **best_lgbm_params)\n",
        "    xgb_clf_opt = XGBClassifier(random_state=42, n_jobs=-1, eval_metric='logloss', use_label_encoder=False, **best_xgb_params)\n",
        "\n",
        "    voting_clf_opt = VotingClassifier(\n",
        "        estimators=[('lgbm', lgbm_clf_opt), ('xgb', xgb_clf_opt)],\n",
        "        voting='soft',\n",
        "        weights=[w1_lgbm, w2_xgb],\n",
        "        n_jobs=-1\n",
        "    )\n",
        "\n",
        "    voting_clf_opt.fit(X_train_smote, y_train_smote)\n",
        "    y_pred = voting_clf_opt.predict(X_val)\n",
        "\n",
        "    return f1_score(y_val, y_pred)\n",
        "\n",
        "print(\"--- [재튜닝] 앙상블 가중치 최적화 시작 ---\")\n",
        "study_voting_weights_re = optuna.create_study(direction='maximize')\n",
        "study_voting_weights_re.optimize(objective_voting_weights_re, n_trials=50, show_progress_bar=True)\n",
        "\n",
        "print(\"\\n[재튜닝 가중치 튜닝] 최적 F1 Score:\", study_voting_weights_re.best_value)\n",
        "print(\"[재튜닝 가중치 튜닝] 최적 가중치 (LGBM:XGB):\", study_voting_weights_re.best_params['w1_lgbm'], \":\", 1.0 - study_voting_weights_re.best_params['w1_lgbm'])\n",
        "\n",
        "best_weights_lgbm = study_voting_weights_re.best_params['w1_lgbm']\n",
        "best_weights_re = [best_weights_lgbm, 1.0 - best_weights_lgbm]\n",
        "\n",
        "final_weighted_ensemble_model_re = VotingClassifier(\n",
        "    estimators=[('lgbm', LGBMClassifier(random_state=42, n_jobs=-1, verbosity=-1, **best_lgbm_params)),\n",
        "                ('xgb', XGBClassifier(random_state=42, n_jobs=-1, eval_metric='logloss', use_label_encoder=False, **best_xgb_params))],\n",
        "    voting='soft',\n",
        "    weights=best_weights_re,\n",
        "    n_jobs=-1\n",
        ")\n",
        "\n",
        "print(\"\\n최적 가중치로 최종 앙상블 모델 재학습 시작...\")\n",
        "final_weighted_ensemble_model_re.fit(X_train_smote, y_train_smote)\n",
        "print(\"최적 가중치로 최종 앙상블 모델 학습 완료.\")"
      ],
      "metadata": {
        "id": "4-aLrrql44et"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "from sklearn.metrics import f1_score\n",
        "from sklearn.ensemble import VotingClassifier\n",
        "from lightgbm import LGBMClassifier\n",
        "from xgboost import XGBClassifier\n",
        "\n",
        "\n",
        "best_lgbm_params = {'n_estimators': 874, 'learning_rate': 0.08326062054708025, 'max_depth': 8, 'num_leaves': 62, 'min_child_samples': 71, 'subsample': 0.9737480587317234, 'colsample_bytree': 0.8917908434852498, 'reg_alpha': 0.013213503671865254, 'reg_lambda': 5.4226824776857555e-05}\n",
        "best_xgb_params = {'n_estimators': 921, 'learning_rate': 0.023703360450466202, 'max_depth': 9, 'min_child_weight': 1, 'gamma': 0.38887847738437764, 'subsample': 0.8954154427644301, 'colsample_bytree': 0.6157837367341784, 'reg_alpha': 0.005934695765543821, 'reg_lambda': 0.0010594014155312593}\n",
        "\n",
        "best_weights_lgbm = 0.5891367693504789\n",
        "best_weights_re = [best_weights_lgbm, 1.0 - best_weights_lgbm]\n",
        "\n",
        "\n",
        "final_weighted_ensemble_model_re = VotingClassifier(\n",
        "    estimators=[('lgbm', LGBMClassifier(random_state=42, n_jobs=-1, verbosity=-1, **best_lgbm_params)),\n",
        "                ('xgb', XGBClassifier(random_state=42, n_jobs=-1, eval_metric='logloss', use_label_encoder=False, **best_xgb_params))],\n",
        "    voting='soft',\n",
        "    weights=best_weights_re,\n",
        "    n_jobs=-1\n",
        ")\n",
        "print(\"최종 앙상블 모델 재학습 시작...\")\n",
        "final_weighted_ensemble_model_re.fit(X_train_smote, y_train_smote)\n",
        "print(\"최종 앙상블 모델 학습 완료.\")\n",
        "\n",
        "\n",
        "final_model = final_weighted_ensemble_model_re\n",
        "\n",
        "y_proba_final = final_model.predict_proba(X_val)[:, 1]\n",
        "\n",
        "best_f1_refined = 0\n",
        "best_threshold_refined = 0.5\n",
        "\n",
        "for threshold in np.arange(0.50, 0.70, 0.001):\n",
        "    y_pred_threshold = (y_proba_final >= threshold).astype(int)\n",
        "    f1 = f1_score(y_val, y_pred_threshold)\n",
        "\n",
        "    if f1 > best_f1_refined:\n",
        "        best_f1_refined = f1\n",
        "        best_threshold_refined = threshold\n",
        "\n",
        "print(f\"\\n--- 최종 임곗값 정밀 튜닝 결과 ---\")\n",
        "print(f\"최적 임곗값: {best_threshold_refined:.3f}\")\n",
        "print(f\"최종 F1 Score: {best_f1_refined:.4f}\")"
      ],
      "metadata": {
        "id": "Bn_WE0IA1FIJ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "import numpy as np\n",
        "\n",
        "\n",
        "original_test_df = pd.read_csv('./test.csv')\n",
        "test_ids_for_submission = original_test_df['id']\n",
        "\n",
        "cols_to_scale = ['Time', 'Amount']\n",
        "\n",
        "original_test_df[cols_to_scale] = scaler.transform(original_test_df[cols_to_scale])\n",
        "\n",
        "X_test_final = original_test_df.drop('id', axis=1)\n",
        "\n",
        "\n",
        "final_model = final_weighted_ensemble_model_re\n",
        "best_threshold = 0.674\n",
        "\n",
        "\n",
        "test_proba = final_model.predict_proba(X_test_final)[:, 1]\n",
        "\n",
        "\n",
        "final_predictions = (test_proba >= best_threshold).astype(int)\n",
        "\n",
        "\n",
        "submission = pd.DataFrame({\n",
        "    \"id\": test_ids_for_submission,\n",
        "    \"Class\": final_predictions\n",
        "})\n",
        "\n",
        "submission.to_csv(\"./submission.csv\", index=False)\n",
        "\n",
        "print(\"\\n--- 최종 submission.csv 파일 생성 완료 ---\")\n"
      ],
      "metadata": {
        "id": "o-acrkhlzynS"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "과적합이 문제라고 생각해서 하이퍼파라미터를 더 느슨하게 만들었는데 오히려 성능이 떨어짐"
      ],
      "metadata": {
        "id": "g3QDZmdg3vik"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# SMOTE를 버리고, LightGBM 단일 모델에 scale_pos_weight를 적용하여 재튜닝"
      ],
      "metadata": {
        "id": "s-dhY_oN34AQ"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "\n",
        "count_neg = (y_train == 0).sum()\n",
        "count_pos = (y_train == 1).sum()\n",
        "\n",
        "\n",
        "SCALE_POS_WEIGHT = count_neg / count_pos\n",
        "\n",
        "print(f\"원본 y_train의 사기 거래 비율: {count_pos / len(y_train) * 100:.3f}%\")\n",
        "print(f\"계산된 LightGBM 클래스 가중치 (SCALE_POS_WEIGHT): {SCALE_POS_WEIGHT:.2f}\")"
      ],
      "metadata": {
        "id": "JjVZy6VA2-MH"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import optuna\n",
        "from lightgbm import LGBMClassifier\n",
        "from sklearn.metrics import f1_score\n",
        "\n",
        "def objective_lgbm_final(trial):\n",
        "    lgbm_params = {\n",
        "        'n_estimators': trial.suggest_int('n_estimators', 500, 1500),\n",
        "        'learning_rate': trial.suggest_loguniform('learning_rate', 0.01, 0.1),\n",
        "        'max_depth': trial.suggest_int('max_depth', 5, 7),\n",
        "        'num_leaves': trial.suggest_int('num_leaves', 31, 50),\n",
        "        'min_child_samples': trial.suggest_int('min_child_samples', 20, 80),\n",
        "        'subsample': trial.suggest_uniform('subsample', 0.7, 1.0),\n",
        "        'colsample_bytree': trial.suggest_uniform('colsample_bytree', 0.7, 1.0),\n",
        "        'reg_alpha': trial.suggest_loguniform('reg_alpha', 1e-5, 5.0),\n",
        "        'reg_lambda': trial.suggest_loguniform('reg_lambda', 1e-5, 5.0),\n",
        "    }\n",
        "\n",
        "    model = LGBMClassifier(\n",
        "        random_state=42,\n",
        "        n_jobs=-1,\n",
        "        verbosity=-1,\n",
        "        scale_pos_weight=SCALE_POS_WEIGHT, # Correctly placed as a keyword argument\n",
        "        **lgbm_params\n",
        "    )\n",
        "\n",
        "    # Use original data for training and validation\n",
        "    model.fit(X_train, y_train)\n",
        "    y_pred = model.predict(X_val)\n",
        "\n",
        "    return f1_score(y_val, y_pred)\n",
        "\n",
        "print(\"--- [최종] LightGBM 단일 모델 최적화 시작 (n_trials=50, 클래스 가중치 적용) ---\")\n",
        "study_lgbm_final = optuna.create_study(direction='maximize')\n",
        "study_lgbm_final.optimize(objective_lgbm_final, n_trials=50, show_progress_bar=True)\n",
        "\n",
        "print(\"\\n[최종 LightGBM] 최적 F1 Score (단일 모델):\", study_lgbm_final.best_value)\n",
        "print(\"[최종 LightGBM] 최적 하이퍼파라미터:\", study_lgbm_final.best_params)\n",
        "\n",
        "# 최적 파라미터 저장\n",
        "best_lgbm_final_params = study_lgbm_final.best_params"
      ],
      "metadata": {
        "id": "gf8ajYA_5HHx"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "new_cell_1"
      },
      "source": [
        "from lightgbm import LGBMClassifier\n",
        "from sklearn.metrics import f1_score, classification_report, accuracy_score, precision_score, recall_score\n",
        "import numpy as np\n",
        "\n",
        "# Instantiate the final LightGBM model with the best parameters and scale_pos_weight\n",
        "final_lgbm_model = LGBMClassifier(\n",
        "    random_state=42,\n",
        "    n_jobs=-1,\n",
        "    verbosity=-1,\n",
        "    scale_pos_weight=SCALE_POS_WEIGHT,\n",
        "    **best_lgbm_final_params\n",
        ")\n",
        "\n",
        "print(\"최종 LightGBM 모델 학습 시작...\")\n",
        "final_lgbm_model.fit(X_train, y_train)\n",
        "print(\"최종 LightGBM 모델 학습 완료.\")\n",
        "\n",
        "# Predict probabilities on the validation set\n",
        "y_proba_final_lgbm = final_lgbm_model.predict_proba(X_val)[:, 1]\n",
        "\n",
        "# Re-tune prediction threshold for the single LightGBM model\n",
        "best_f1_lgbm_single = 0\n",
        "best_threshold_lgbm_single = 0.5\n",
        "\n",
        "for threshold in np.arange(0.01, 0.99, 0.01):\n",
        "    y_pred_threshold_lgbm = (y_proba_final_lgbm >= threshold).astype(int)\n",
        "    f1 = f1_score(y_val, y_pred_threshold_lgbm)\n",
        "\n",
        "    if f1 > best_f1_lgbm_single:\n",
        "        best_f1_lgbm_single = f1\n",
        "        best_threshold_lgbm_single = threshold\n",
        "\n",
        "print(f\"\\n--- 최종 LightGBM 단일 모델 성능 평가 (최적 임곗값 적용) ---\")\n",
        "print(f\"최적 임곗값: {best_threshold_lgbm_single:.2f}\")\n",
        "print(f\"최종 F1 Score: {best_f1_lgbm_single:.4f}\")\n",
        "\n",
        "# Evaluate with the best threshold\n",
        "y_pred_final_lgbm = (y_proba_final_lgbm >= best_threshold_lgbm_single).astype(int)\n",
        "\n",
        "accuracy = accuracy_score(y_val, y_pred_final_lgbm)\n",
        "precision = precision_score(y_val, y_pred_final_lgbm)\n",
        "recall = recall_score(y_val, y_pred_final_lgbm)\n",
        "f1 = f1_score(y_val, y_pred_final_lgbm)\n",
        "\n",
        "print(f\"Accuracy: {accuracy:.4f}\")\n",
        "print(f\"Precision: {precision:.4f}\")\n",
        "print(f\"Recall: {recall:.4f}\")\n",
        "print(f\"F1 Score: {f1:.4f}\")\n",
        "\n",
        "print(\"\\n[Classification Report]\")\n",
        "print(classification_report(y_val, y_pred_final_lgbm, target_names=['Normal (0)', 'Fraud (1)']))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "original_test_df = pd.read_csv('./test.csv')\n",
        "test_ids_for_submission = original_test_df['id']\n",
        "cols_to_scale = ['Time', 'Amount']\n",
        "\n",
        "# test_df에 동일한 스케일러 적용 (fit 없이 transform만!)\n",
        "original_test_df[cols_to_scale] = scaler.transform(original_test_df[cols_to_scale])\n",
        "X_test_final = original_test_df.drop('id', axis=1)\n",
        "\n",
        "# 2. 최종 예측 수행 및 최적 임곗값 적용\n",
        "final_model_single = final_lgbm_model # 최종 LightGBM 모델 사용\n",
        "best_threshold = best_threshold_lgbm_single # LightGBM 단일 모델의 최적 임곗값 사용\n",
        "\n",
        "test_proba = final_model_single.predict_proba(X_test_final)[:, 1]\n",
        "final_predictions = (test_proba >= best_threshold).astype(int)\n",
        "\n",
        "# 3. submission.csv 파일 생성\n",
        "submission = pd.DataFrame({\n",
        "    \"id\": test_ids_for_submission,\n",
        "    \"Class\": final_predictions\n",
        "})\n",
        "\n",
        "submission.to_csv(\"./submission2.csv\", index=False)\n",
        "\n",
        "print(\"\\n--- 최종 submission.csv 파일 생성 완료 ---\")"
      ],
      "metadata": {
        "id": "H6DX0ocs_-Y4"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "오버샘플링이 문제였던것같다..제일 효과가 좋았다"
      ],
      "metadata": {
        "id": "oQGo6PWBBbX6"
      }
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "fAyqgjrJB8YY"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "eP2VmJlZA8M-"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}