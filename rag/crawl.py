import os
import time
import json
import csv
from dotenv import load_dotenv
from openai import OpenAI
from playwright.sync_api import sync_playwright

# 1. í™˜ê²½ ë³€ìˆ˜ ë° ì„¤ì •
load_dotenv()
openai_api_key = os.getenv("OPENAI_API_KEY")

if not openai_api_key:
    print("âŒ .env íŒŒì¼ì— 'OPENAI_API_KEY'ê°€ ì—†ìŠµë‹ˆë‹¤.")
    exit()

client = OpenAI(api_key=openai_api_key)

# --- [ìˆ˜ì • í¬ì¸íŠ¸ 1] ëª©í‘œ ê°œìˆ˜ë¥¼ 200& C:/Users/user/anaconda3/envs/ds6_RAG/python.exe crawl.pyìœ¼ë¡œ í™•ì‹¤íˆ ì„¤ì • ---
TARGET_CATEGORY_URL = "https://www.daisomall.co.kr/ds/exhCtgr/C208/CTGR_00014/CTGR_00057/CTGR_00366"
MAX_ITEMS = 209  
CSV_FILE = "daiso_analysis_result.csv"
HEADERS = ["ìƒí’ˆëª…", "URL", "í¡ìˆ˜ë ¥", "ë³´ìŠµë ¥", "ìê·¹ë„", "í•œì¤„ìš”ì•½"]

def get_product_links(page):
    """ì§€ì—° ë¡œë”©ì„ ê·¹ë³µí•˜ê³  200ê°œì˜ ë§í¬ë¥¼ ìˆ˜ì§‘í•  ë•Œê¹Œì§€ ì •ë°€ ìŠ¤í¬ë¡¤í•©ë‹ˆë‹¤."""
    print(f"ğŸ“‚ ì¹´í…Œê³ ë¦¬ í˜ì´ì§€ ì ‘ì† ì¤‘: {TARGET_CATEGORY_URL}")
    page.goto(TARGET_CATEGORY_URL, wait_until="networkidle")
    
    unique_links = set()
    prev_count = 0
    no_change_count = 0
    max_no_change = 7 # 7ë²ˆ ì‹œë„í•´ë„ ì•ˆ ëŠ˜ì–´ë‚˜ë©´ ì§„ì§œ ëì„

    print(f"ğŸ“œ ëª©í‘œ ê°œìˆ˜({MAX_ITEMS}ê°œ) ìˆ˜ì§‘ì„ ì‹œì‘í•©ë‹ˆë‹¤. (20ê°œ ë‹¨ìœ„ë¡œ ë¡œë”©ë¨)")

    while len(unique_links) < MAX_ITEMS:
        # ë§ˆìš°ìŠ¤ íœ ì„ êµ´ë ¤ ì‹¤ì œ ì‚¬ìš©ìê°€ ë‚´ë¦¬ëŠ” ê²ƒì²˜ëŸ¼ ì‹œë®¬ë ˆì´ì…˜
        for _ in range(8): 
            page.mouse.wheel(0, 3000) 
            time.sleep(0.7)
        
        time.sleep(2.5) # ë¡œë”© ëŒ€ê¸° ì‹œê°„ ì¶©ë¶„íˆ í™•ë³´
        
        current_links = page.evaluate("""
            () => {
                const anchors = Array.from(document.querySelectorAll('a'));
                return anchors
                    .map(a => a.href)
                    .filter(href => href && href.includes('/pd/pdr/'));
            }
        """)
        
        for link in current_links:
            unique_links.add(link)
        
        current_count = len(unique_links)
        print(f"   ğŸ”„ í˜„ì¬ í™•ë³´ëœ ë§í¬: {current_count}ê°œ / {MAX_ITEMS}ê°œ")

        # ë” ì´ìƒ ì•ˆ ëŠ˜ì–´ë‚˜ëŠ”ì§€ ì²´í¬
        if current_count == prev_count:
            no_change_count += 1
            if no_change_count >= max_no_change:
                print("âš ï¸ ë” ì´ìƒ ìƒˆë¡œìš´ ìƒí’ˆì´ ì—†ìŠµë‹ˆë‹¤. ìˆ˜ì§‘ì„ ì¢…ë£Œí•©ë‹ˆë‹¤.")
                break
        else:
            no_change_count = 0
            
        prev_count = current_count

    final_links = list(unique_links)[:MAX_ITEMS]
    print(f"âœ… {len(final_links)}ê°œì˜ ìƒí’ˆì„ ìˆ˜ì§‘í•  ì˜ˆì •ì…ë‹ˆë‹¤.") # ì´ ë©”ì‹œì§€ê°€ 200ìœ¼ë¡œ ë‚˜ì™€ì•¼ í•¨!
    return final_links

def analyze_with_gpt(text):
    prompt = f"""
    ë‹¹ì‹ ì€ í™”ì¥í’ˆ ë¦¬ë·° ë°ì´í„° ë¶„ì„ê°€ì…ë‹ˆë‹¤. 
    ì œê³µëœ í…ìŠ¤íŠ¸ì—ì„œ ì œí’ˆì˜ 'ë¦¬ë·° í†µê³„' ìˆ˜ì¹˜ì™€ 'í•µì‹¬ ìš”ì•½'ì„ ì¶”ì¶œí•˜ì—¬ JSON í˜•ì‹ìœ¼ë¡œ ë‹µë³€í•˜ì„¸ìš”.
    í•­ëª©: í¡ìˆ˜ë ¥, ë³´ìŠµë ¥, ìê·¹ë„, í•œì¤„ìš”ì•½
    í…ìŠ¤íŠ¸: {text[:8000]}
    """
    try:
        response = client.chat.completions.create(
            model="gpt-4o-mini",
            messages=[
                {"role": "system", "content": "JSON í˜•ì‹ìœ¼ë¡œë§Œ ë‹µë³€í•˜ê³  ì •ë³´ ì—†ìœ¼ë©´ 'ì •ë³´ì—†ìŒ'ìœ¼ë¡œ í‘œì‹œ."},
                {"role": "user", "content": prompt}
            ],
            response_format={"type": "json_object"}
        )
        return json.loads(response.choices[0].message.content)
    except:
        return {}

def main():
    with sync_playwright() as p:
        browser = p.chromium.launch(headless=True) 
        context = browser.new_context(user_agent="Mozilla/5.0...")
        page = context.new_page()

        # ë§í¬ ìˆ˜ì§‘
        product_links = get_product_links(page)
        
        if not os.path.exists(CSV_FILE):
            with open(CSV_FILE, "w", encoding="utf-8-sig", newline="") as f:
                writer = csv.DictWriter(f, fieldnames=HEADERS)
                writer.writeheader()

        # ë¶„ì„ ë° ì €ì¥
        for idx, link in enumerate(product_links):
            print(f"[{idx+1}/{len(product_links)}] ì²˜ë¦¬ ì¤‘: {link}")
            try:
                page.goto(link, wait_until="domcontentloaded")
                time.sleep(2)
                page.evaluate("window.scrollTo(0, document.body.scrollHeight)")
                
                content = page.inner_text("body")
                title = page.title().replace("ë‹¤ì´ì†Œëª°", "").strip()
                analyzed = analyze_with_gpt(content)
                
                row = {
                    "ìƒí’ˆëª…": title, "URL": link,
                    "í¡ìˆ˜ë ¥": analyzed.get("í¡ìˆ˜ë ¥", "ì •ë³´ì—†ìŒ"),
                    "ë³´ìŠµë ¥": analyzed.get("ë³´ìŠµë ¥", "ì •ë³´ì—†ìŒ"),
                    "ìê·¹ë„": analyzed.get("ìê·¹ë„", "ì •ë³´ì—†ìŒ"),
                    "í•œì¤„ìš”ì•½": analyzed.get("í•œì¤„ìš”ì•½", "ì •ë³´ì—†ìŒ")
                }

                with open(CSV_FILE, "a", encoding="utf-8-sig", newline="") as f:
                    writer = csv.DictWriter(f, fieldnames=HEADERS)
                    writer.writerow(row)
                print(f"   âœ”ï¸ ì €ì¥ ì™„ë£Œ: {row['í•œì¤„ìš”ì•½'][:30]}...")
            except Exception as e:
                print(f"   âŒ ì˜¤ë¥˜: {e}")
            time.sleep(2.5)

        browser.close()
        print(f"ğŸ‰ ì™„ë£Œ! '{CSV_FILE}'ë¥¼ í™•ì¸í•˜ì„¸ìš”.")

if __name__ == "__main__":
    main()