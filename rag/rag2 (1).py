import streamlit as st
import pandas as pd
import os
import time
import re
from dotenv import load_dotenv
from langchain_openai import ChatOpenAI, OpenAIEmbeddings
from langchain_community.vectorstores import Chroma
from langchain_core.prompts import PromptTemplate
from langchain_core.output_parsers import StrOutputParser
from langchain_core.runnables import RunnablePassthrough
from langchain_core.documents import Document
from rank_bm25 import BM25Okapi

# ---------------------------------------------------------
# 1. í™˜ê²½ ì„¤ì •
# ---------------------------------------------------------
current_dir = os.path.dirname(os.path.abspath(__file__))
env_path = os.path.join(current_dir, '.env')
load_dotenv(dotenv_path=env_path)

# ---------------------------------------------------------
# 2. í”„ë¦¬ë¯¸ì—„ ë””ìì¸ ë° UI/UX (ë‹¤ì´ì†Œ ë ˆë“œ ì•„ì´ë´í‹°í‹°)
# ---------------------------------------------------------
st.set_page_config(page_title="ë‹¤ì´ì†Œ ë·°í‹° íë ˆì´í„°", layout="wide")

st.markdown("""
    <style>
    /* ë‹¤ì´ì†Œ ë ˆë“œ ê·¸ë¼ë°ì´ì…˜ ë°°ë„ˆ */
    .welcome-banner {
        background: linear-gradient(135deg, #FF1535 0%, #FF4D6D 100%);
        padding: 45px;
        border-radius: 20px;
        color: white;
        text-align: center;
        margin-bottom: 35px;
        box-shadow: 0 10px 30px rgba(255, 21, 53, 0.25);
    }
    .welcome-banner h1 { margin: 0; font-size: 2.8rem; font-weight: 900; letter-spacing: -1px; }
    .welcome-banner p { margin: 15px 0 0; font-size: 1.2rem; font-weight: 300; opacity: 0.95; }

    /* ì „ë¬¸ê°€ í”„ë¡œí•„ ì„¹ì…˜ */
    .expert-intro {
        display: flex;
        align-items: center;
        gap: 25px;
        background-color: white;
        padding: 25px;
        border-radius: 20px;
        border: 1px solid #f1f3f5;
        margin-bottom: 30px;
    }
    .expert-avatar {
        font-size: 50px;
        background: #FFF0F3;
        width: 85px;
        height: 85px;
        display: flex;
        align-items: center;
        justify-content: center;
        border-radius: 50%;
        border: 2px solid #FF1535;
    }
    .expert-text { font-size: 1.2rem; color: #2d3436; font-weight: 600; line-height: 1.6; }

    /* ì œí’ˆëª… ëª…ì‹œëœ ì¢…í•© ë¦¬ë·° íƒ€ì›í˜• ë°•ìŠ¤ */
    .summary-box {
        background-color: #F8F9FA;
        border: 1px solid #FF1535;
        border-radius: 100px;
        padding: 15px 35px;
        margin-top: 15px;
        margin-bottom: 25px;
        display: inline-flex;
        align-items: center;
        max-width: 95%;
        box-shadow: 2px 2px 10px rgba(255, 21, 53, 0.08);
    }
    .summary-label { 
        font-weight: 800; color: #FF1535; margin-right: 20px; min-width: 160px; 
        border-right: 2px solid #FF1535; padding-right: 15px; font-size: 0.95rem; text-align: center;
    }
    .summary-text { color: #333; font-size: 1rem; font-weight: 500; font-style: italic; }
    .section-title { font-weight: bold; color: #FF1535; margin: 30px 0 10px 0; font-size: 1.25rem; display: block; }
    
    /* ë² ìŠ¤íŠ¸ ì¶”ì²œ ë°•ìŠ¤ (ê³¨ë“œ í…Œë§ˆ) */
    .best-box {
        background: linear-gradient(135deg, #FFF8E7 0%, #FFFAF0 100%);
        border: 2px solid #FFD700;
        border-radius: 15px;
        padding: 20px 30px;
        margin-top: 25px;
        display: block;
        max-width: 100%;
        box-shadow: 0 4px 15px rgba(255, 215, 0, 0.25);
    }
    .best-title {
        font-weight: 900; color: #B8860B; font-size: 1.1rem; margin-bottom: 10px;
        display: flex; align-items: center; gap: 8px;
    }
    .best-text { color: #5D4E37; font-size: 1.05rem; font-weight: 600; }
    </style>
    """, unsafe_allow_html=True)

# ---------------------------------------------------------
# 3. ë°ì´í„° ë¡œë“œ ë° ë²¡í„° ì €ì¥ì†Œ 
# ---------------------------------------------------------
@st.cache_resource
def get_vectorstore():
    persist_directory = os.path.join(current_dir, "chroma_db_v2")
    embeddings = OpenAIEmbeddings(model="text-embedding-3-small",chunk_size=1000)
    
    # 1. ì´ë¯¸ ì €ì¥ëœ DBê°€ ìˆëŠ”ì§€ í™•ì¸
    if os.path.exists(persist_directory) and os.listdir(persist_directory):
        return Chroma(persist_directory=persist_directory, embedding_function=embeddings)
    
    # 2. DBê°€ ì—†ìœ¼ë©´ ìƒˆë¡œ ìƒì„±
    documents = []
    csv_file = os.path.join(current_dir, "final_integrated_data_v2.csv")
    
    if os.path.exists(csv_file):
        df = pd.read_csv(csv_file, encoding="utf-8-sig")
        
        for _, row in df.iterrows():
            content_parts = []
            for col in df.columns:
                val = row[col]
                if pd.notna(val):
                    content_parts.append(f"{col}: {val}")
            page_content = "\n".join(content_parts)
            
            documents.append(Document(
                page_content=page_content, 
                metadata={"source": csv_file, "row": _}
            ))
    
    if not documents:
        st.error("ë°ì´í„° íŒŒì¼ì„ ì°¾ì„ ìˆ˜ ì—†ìŠµë‹ˆë‹¤.")
        st.stop()
    
    # ë°°ì¹˜ ì²˜ë¦¬ë¡œ ë²¡í„° ì €ì¥ì†Œ ìƒì„± (í† í° ì œí•œ ìš°íšŒ)
    vectorstore = Chroma.from_documents(
        documents=documents[:50],
        embedding=embeddings,
        persist_directory=persist_directory
    )
    
    # ë‚˜ë¨¸ì§€ ì¶”ê°€
    batch_size = 50
    for i in range(50, len(documents), batch_size):
        batch = documents[i : i + batch_size]
        vectorstore.add_documents(batch)
    
    return vectorstore

# API í‚¤ í™•ì¸
if "OPENAI_API_KEY" not in os.environ:
    api_key = st.sidebar.text_input("OpenAI API Keyë¥¼ ì…ë ¥í•˜ì„¸ìš”", type="password")
    if api_key:
        os.environ["OPENAI_API_KEY"] = api_key
    else:
        st.sidebar.warning("ğŸ”‘ API Keyë¥¼ ì…ë ¥í•´ ì£¼ì„¸ìš”.")
        st.stop()

vectorstore = get_vectorstore()

# ---------------------------------------------------------
# 4-1. BM25 ì¸ë±ìŠ¤ ìƒì„±
# ---------------------------------------------------------
def simple_tokenizer(text):
    if not isinstance(text, str): return []
    text = re.sub(r'[_\-().]', ' ', text)
    tokens = re.findall(r'[ê°€-í£a-zA-Z0-9]{1,}', text) 
    return tokens

@st.cache_resource
def get_bm25_index():
    """BM25 ê²€ìƒ‰ì„ ìœ„í•œ ì¸ë±ìŠ¤ ìƒì„±"""
    csv_file = os.path.join(current_dir, "final_integrated_data_v2.csv")
    
    if not os.path.exists(csv_file):
        return None, None
    
    df = pd.read_csv(csv_file, encoding="utf-8-sig")
    
    # ë¬¸ì„œ í…ìŠ¤íŠ¸ ìƒì„±
    documents = []
    doc_texts = []
    
    for _, row in df.iterrows():
        content_parts = []
        # ê²€ìƒ‰ì— ìœ ìš©í•œ ì»¬ëŸ¼ë§Œ ê²°í•©
        cols_to_use = ['ìƒí’ˆëª…', 'ìƒì„¸ì •ë³´', 'í•œì¤„ìš”ì•½']
        for col in cols_to_use:
            if col in df.columns:
                val = row[col]
                if pd.notna(val):
                    content_parts.append(str(val))
        text = " ".join(content_parts)
        documents.append(text)
        doc_texts.append(text)
    
    # í† í°í™”
    tokenized_docs = [simple_tokenizer(doc) for doc in doc_texts]
    
    # BM25 ì¸ë±ìŠ¤ ìƒì„±
    bm25 = BM25Okapi(tokenized_docs)
    
    return bm25, documents

bm25_index, bm25_documents = get_bm25_index()

# ---------------------------------------------------------
# 4-2. ì¿¼ë¦¬ ë¦¬ë¼ì´íŒ… í•¨ìˆ˜
# ---------------------------------------------------------
@st.cache_data(ttl=3600)  # 1ì‹œê°„ ìºì‹±
def rewrite_query(original_query: str) -> str:
    """ì‚¬ìš©ì ì§ˆë¬¸ì„ ê²€ìƒ‰ì— ìµœì í™”ëœ í˜•íƒœë¡œ ì¬ì‘ì„±"""
    rewrite_prompt = f"""ì‚¬ìš©ìì˜ ì§ˆë¬¸ì„ í™”ì¥í’ˆ ê²€ìƒ‰ì— ìµœì í™”ëœ í‚¤ì›Œë“œë¡œ ì¬ì‘ì„±í•˜ì„¸ìš”.

[ê·œì¹™]
1. êµ¬ì–´ì²´ë¥¼ ê²€ìƒ‰ í‚¤ì›Œë“œë¡œ ë³€í™˜
2. ì˜¤íƒ€ê°€ ìˆìœ¼ë©´ êµì • (ì–´ì„ ì´ˆâ†’ì–´ì„±ì´ˆ, ì‹œì¹´ê·¸ë¦¼â†’ì‹œì¹´í¬ë¦¼)
3. ê³„ì ˆ/ìƒí™©ì„ íš¨ëŠ¥ í‚¤ì›Œë“œë¡œ ë³€í™˜ (ê²¨ìš¸â†’ë³´ìŠµ, ê±´ì¡° ë°©ì§€)
4. ì„±ë³„ í‘œí˜„ì„ ëª…í™•íˆ (ë‚¨ì¹œâ†’ë‚¨ì„±ìš©, ë‚¨ì)
5. ì›ë˜ ì§ˆë¬¸ì˜ í•µì‹¬ ì˜ë„ë¥¼ ìœ ì§€
6. **ì œí’ˆëª… í‚¤ì›Œë“œëŠ” ë„ì–´ì“°ê¸° ë²„ì „ê³¼ ë¶™ì—¬ì“°ê¸° ë²„ì „ ëª¨ë‘ í¬í•¨**
   ì˜ˆ: "PDRN ì½œë¼ê² í† ë„ˆ" â†’ "PDRN ì½œë¼ê² í† ë„ˆ PDRNì½œë¼ê²í† ë„ˆ"
7. ì¬ì‘ì„±ëœ ì§ˆë¬¸ë§Œ ì¶œë ¥ (ì„¤ëª… ì—†ì´)

ì›ë˜ ì§ˆë¬¸: {original_query}
ì¬ì‘ì„±ëœ ì§ˆë¬¸:"""
    
    try:
        rewrite_llm = ChatOpenAI(model_name="gpt-4o-mini", temperature=0)
        result = rewrite_llm.invoke(rewrite_prompt)
        rewritten = result.content.strip()
        return rewritten if rewritten else original_query
    except:
        return original_query

# ---------------------------------------------------------
# 4-3. ì¹´í…Œê³ ë¦¬ í‚¤ì›Œë“œ ì •ì˜ ë° ì¶”ì¶œ í•¨ìˆ˜
# ---------------------------------------------------------
CATEGORY_KEYWORDS = {
    # ì œí’ˆ ìœ í˜• í‚¤ì›Œë“œ (ì§ˆë¬¸ì—ì„œ ì¶”ì¶œí•  í‚¤ì›Œë“œ: ìƒí’ˆëª…ì—ì„œ ë§¤ì¹­í•  í‚¤ì›Œë“œë“¤)
    "íŒ¨ë“œ": ["íŒ¨ë“œ", "pad"],
    "í† ë„ˆ": ["í† ë„ˆ", "ìŠ¤í‚¨", "toner"],
    "í¬ë¦¼": ["í¬ë¦¼", "cream"],
    "ë¡œì…˜": ["ë¡œì…˜", "lotion"],
    "ì„¸ëŸ¼": ["ì„¸ëŸ¼", "ì—ì„¼ìŠ¤", "ì•°í”Œ", "serum", "essence", "ampoule"],
    "í´ë Œì§•": ["í´ë Œì§•", "í´ë Œì €", "í¼", "ì›Œì‹œ", "cleansing", "cleanser", "foam", "wash"],
    "ë§ˆìŠ¤í¬": ["ë§ˆìŠ¤í¬", "íŒ©", "mask", "pack"],
    "ë¯¸ìŠ¤íŠ¸": ["ë¯¸ìŠ¤íŠ¸", "ìŠ¤í”„ë ˆì´", "mist", "spray"],
    "ì˜¤ì¼": ["ì˜¤ì¼", "oil"],
    "ì„ í¬ë¦¼": ["ì„ í¬ë¦¼", "ì„ ë¸”ë¡", "ìì™¸ì„ ", "sunscreen", "sun block", "spf"],
    "ë¦½": ["ë¦½", "lip"],
    "ì•„ì´": ["ì•„ì´í¬ë¦¼", "ì•„ì´ì„¸ëŸ¼", "eye"],
}

def extract_category_from_query(query: str) -> list:
    """ì§ˆë¬¸ì—ì„œ ì œí’ˆ ì¹´í…Œê³ ë¦¬ í‚¤ì›Œë“œ ì¶”ì¶œ"""
    query_lower = query.lower()
    found_categories = []
    
    for category, keywords in CATEGORY_KEYWORDS.items():
        # ì§ˆë¬¸ì— ì¹´í…Œê³ ë¦¬ í‚¤ì›Œë“œê°€ í¬í•¨ë˜ì–´ ìˆëŠ”ì§€ í™•ì¸
        if category in query_lower:
            found_categories.append(category)
        # ë™ì˜ì–´ë„ ì²´í¬
        for kw in keywords:
            if kw in query_lower and category not in found_categories:
                found_categories.append(category)
                break
    
    return found_categories

# ---------------------------------------------------------
# 4-4. ì„±ë¶„ í‚¤ì›Œë“œ ì •ì˜ ë° ì¶”ì¶œ í•¨ìˆ˜
# ---------------------------------------------------------
INGREDIENT_KEYWORDS = [
    # ì£¼ìš” í™”ì¥í’ˆ ì„±ë¶„ ëª©ë¡
    "ì–´ì„±ì´ˆ", "ì‹œì¹´", "ë§ˆë°ì¹´", "ì„¼í…”ë¼", "í‹°íŠ¸ë¦¬", "ë…¹ì°¨", "ë³‘í’€",
    "íˆì•Œë£¨ë¡ ", "íˆì•Œë£¨ë¡ ì‚°", "ì½œë¼ê²", "ë ˆí‹°ë†€", "ë¹„íƒ€ë¯¼", "ë‚˜ì´ì•„ì‹ ì•„ë§ˆì´ë“œ",
    "ì„¸ë¼ë§ˆì´ë“œ", "í©íƒ€ì´ë“œ", "ê¸€ë£¨íƒ€ì¹˜ì˜¨", "ì•„ë¥´ë¶€í‹´", "ì•ŒíŒŒí•˜ì´ë“œë¡ì‹œ",
    "ì‚´ë¦¬ì‹¤ì‚°", "aha", "bha", "pha", "pdrn", "í”„ë¡œí´ë¦¬ìŠ¤",
    "ìŠ¤ì¿ ì•Œë€", "í˜¸í˜¸ë°”", "ì•„ë¥´ê°„", "ë¡œì¦ˆí™", "ìŒ€", "ê¿€", "ë‹¬íŒ½ì´",
    "ì•Œë¡œì—", "ì¹´ë°", "ì¹¼ë¼ë¯¼", "ì§„ì •", "ìˆ˜ë¶„", "ë³´ìŠµ",
]

def extract_ingredients_from_query(query: str) -> list:
    """ì§ˆë¬¸ì—ì„œ ì„±ë¶„ í‚¤ì›Œë“œ ì¶”ì¶œ"""
    query_lower = query.lower()
    found_ingredients = []
    
    for ingredient in INGREDIENT_KEYWORDS:
        if ingredient.lower() in query_lower:
            found_ingredients.append(ingredient.lower())
    
    return found_ingredients

def doc_matches_ingredients(doc_content: str, ingredients: list) -> bool:
    """ë¬¸ì„œê°€ ì§€ì •ëœ ì„±ë¶„ì„ í¬í•¨í•˜ëŠ”ì§€ í™•ì¸ (ìƒí’ˆëª… ë˜ëŠ” ìƒì„¸ì •ë³´)"""
    if not ingredients:
        return True  # ì„±ë¶„ ì§€ì • ì—†ìœ¼ë©´ ëª¨ë“  ë¬¸ì„œ í†µê³¼
    
    doc_lower = doc_content.lower()
    
    # ëª¨ë“  ìš”ì²­ ì„±ë¶„ì´ ë¬¸ì„œì— í¬í•¨ë˜ì–´ì•¼ í•¨ (AND ì¡°ê±´)
    for ingredient in ingredients:
        if ingredient not in doc_lower:
            return False
    
    return True

# ---------------------------------------------------------
# 4-5. í”¼ë¶€ íƒ€ì… í‚¤ì›Œë“œ ì •ì˜ ë° ì¶”ì¶œ í•¨ìˆ˜
# ---------------------------------------------------------
SKIN_TYPE_KEYWORDS = {
    "ê±´ì„±": ["ê±´ì„±", "ê±´ì¡°", "dry"],
    "ì§€ì„±": ["ì§€ì„±", "ì˜¤ì¼ë¦¬", "oily", "í”¼ì§€"],
    "ë³µí•©ì„±": ["ë³µí•©ì„±", "combination"],
    "ë¯¼ê°ì„±": ["ë¯¼ê°", "sensitive", "ì•½ì‚°ì„±"],
    "íŠ¸ëŸ¬ë¸”": ["íŠ¸ëŸ¬ë¸”", "ì—¬ë“œë¦„", "acne", "trouble"],
}

def extract_skin_types_from_query(query: str) -> list:
    """ì§ˆë¬¸ì—ì„œ í”¼ë¶€ íƒ€ì… í‚¤ì›Œë“œ ì¶”ì¶œ"""
    query_lower = query.lower()
    found_types = []
    
    for skin_type, keywords in SKIN_TYPE_KEYWORDS.items():
        for kw in keywords:
            if kw in query_lower and skin_type not in found_types:
                found_types.append(skin_type)
                break
    
    return found_types

def doc_matches_skin_types(doc_content: str, skin_types: list) -> bool:
    """ë¬¸ì„œê°€ ì§€ì •ëœ í”¼ë¶€ íƒ€ì…ì— ì í•©í•œì§€ í™•ì¸ (ìƒì„¸ì •ë³´ì—ì„œ í™•ì¸)"""
    if not skin_types:
        return True  # í”¼ë¶€ íƒ€ì… ì§€ì • ì—†ìœ¼ë©´ ëª¨ë“  ë¬¸ì„œ í†µê³¼
    
    doc_lower = doc_content.lower()
    
    # ìš”ì²­í•œ í”¼ë¶€ íƒ€ì… ì¤‘ í•˜ë‚˜ë¼ë„ ë§¤ì¹­ë˜ë©´ í†µê³¼ (OR ì¡°ê±´)
    for skin_type in skin_types:
        if skin_type in SKIN_TYPE_KEYWORDS:
            for keyword in SKIN_TYPE_KEYWORDS[skin_type]:
                if keyword in doc_lower:
                    return True
    
    return False

def doc_matches_category(doc_content: str, categories: list) -> bool:
    """ë¬¸ì„œê°€ ì§€ì •ëœ ì¹´í…Œê³ ë¦¬ì— í•´ë‹¹í•˜ëŠ”ì§€ í™•ì¸ (ìƒí’ˆëª…ì—ì„œë§Œ ë§¤ì¹­)"""
    if not categories:
        return True  # ì¹´í…Œê³ ë¦¬ ì§€ì • ì—†ìœ¼ë©´ ëª¨ë“  ë¬¸ì„œ í†µê³¼
    
    # ìƒí’ˆëª… ì¶”ì¶œ (ìƒí’ˆëª…ì—ì„œë§Œ ì¹´í…Œê³ ë¦¬ í™•ì¸)
    product_name = ""
    name_match = re.search(r'ìƒí’ˆëª…:\s*([^\n]+)', doc_content)
    if name_match:
        product_name = name_match.group(1).lower()
    
    if not product_name:
        return False  # ìƒí’ˆëª…ì„ ì°¾ì„ ìˆ˜ ì—†ìœ¼ë©´ ì œì™¸
    
    for category in categories:
        if category in CATEGORY_KEYWORDS:
            for keyword in CATEGORY_KEYWORDS[category]:
                # ìƒí’ˆëª…ì— ì¹´í…Œê³ ë¦¬ í‚¤ì›Œë“œê°€ ìˆëŠ”ì§€ë§Œ í™•ì¸ (ì—„ê²©í•œ í•„í„°ë§)
                if keyword in product_name:
                    return True
    
    return False

# ---------------------------------------------------------
# 4-6. í•˜ì´ë¸Œë¦¬ë“œ ê²€ìƒ‰ (ì¿¼ë¦¬ ë¦¬ë¼ì´íŒ… + ë²¡í„° + BM25 + ì¹´í…Œê³ ë¦¬/ì„±ë¶„/í”¼ë¶€íƒ€ì… í•„í„°ë§)
# ---------------------------------------------------------
def get_advanced_context(query, k=15):
    """
    í•˜ì´ë¸Œë¦¬ë“œ ê²€ìƒ‰: ì¿¼ë¦¬ ë¦¬ë¼ì´íŒ… + ë²¡í„°(70%) + BM25(30%)
    + ì¹´í…Œê³ ë¦¬ í•„í„°ë§ + ì„±ë¶„ í•„í„°ë§ + í”¼ë¶€ íƒ€ì… í•„í„°ë§ + ë¦¬ë·°ìˆ˜ ìš°ì„  ì •ë ¬
    """
    
    # 0. ì§ˆë¬¸ì—ì„œ ì¹´í…Œê³ ë¦¬, ì„±ë¶„, í”¼ë¶€ íƒ€ì… ì¶”ì¶œ
    requested_categories = extract_category_from_query(query)
    requested_ingredients = extract_ingredients_from_query(query)
    requested_skin_types = extract_skin_types_from_query(query)
    
    # 1. ì¿¼ë¦¬ ë¦¬ë¼ì´íŒ…
    rewritten_query = rewrite_query(query)
    
    # í•„í„°ë§ì„ ìœ„í•´ ë” ë§ì€ í›„ë³´êµ° í™•ë³´ (10ë°°ìˆ˜)
    has_filters = requested_categories or requested_ingredients or requested_skin_types
    retrieval_k = k * 10 if has_filters else k
    
    # 2. ë²¡í„° ê²€ìƒ‰ (Dense)
    vector_docs = vectorstore.similarity_search(rewritten_query, k=retrieval_k)
    
    # 3. BM25 ê²€ìƒ‰ (Sparse)
    bm25_results = []
    if bm25_index and bm25_documents:
        tokenized_query = simple_tokenizer(rewritten_query)
        bm25_scores = bm25_index.get_scores(tokenized_query)
        
        # ìƒìœ„ kê°œ ì¸ë±ìŠ¤
        top_indices = sorted(range(len(bm25_scores)), key=lambda i: bm25_scores[i], reverse=True)[:retrieval_k]
        bm25_results = [(bm25_documents[i], bm25_scores[i]) for i in top_indices]
    
    # 4. í•˜ì´ë¸Œë¦¬ë“œ ì ìˆ˜ ê³„ì‚°
    doc_scores = {}  # {ë¬¸ì„œë‚´ìš©: (ë²¡í„°ì ìˆ˜, BM25ì ìˆ˜, ì›ë³¸ë¬¸ì„œ)}
    
    # ë²¡í„° ê²°ê³¼ ì¶”ê°€ (ìˆœìœ„ ê¸°ë°˜ ì ìˆ˜: 1ìœ„=1.0, 2ìœ„=0.9...)
    for rank, doc in enumerate(vector_docs):
        vector_score = 1.0 - (rank * 0.02)  # ë” ì„¸ë°€í•œ ì ìˆ˜ ì°¨ì´
        doc_scores[doc.page_content] = {
            'vector': vector_score,
            'bm25': 0,
            'doc': doc
        }
    
    # BM25 ê²°ê³¼ ë³‘í•©
    if bm25_results:
        max_bm25 = max(score for _, score in bm25_results) if bm25_results else 1
        for doc_text, score in bm25_results:
            normalized_bm25 = score / max_bm25 if max_bm25 > 0 else 0
            if doc_text in doc_scores:
                doc_scores[doc_text]['bm25'] = normalized_bm25
            else:
                doc_scores[doc_text] = {
                    'vector': 0,
                    'bm25': normalized_bm25,
                    'doc': Document(page_content=doc_text)
                }
    
    # 5. ì¹´í…Œê³ ë¦¬ + ì„±ë¶„ + í”¼ë¶€ íƒ€ì… í•„í„°ë§ + ì ìˆ˜ ê³„ì‚°
    final_scored = []
    for doc_text, scores in doc_scores.items():
        doc = scores['doc']
        
        # â˜… ì¹´í…Œê³ ë¦¬ í•„í„°ë§: ìš”ì²­í•œ ì¹´í…Œê³ ë¦¬ì— ë§¤ì¹­ë˜ëŠ” ë¬¸ì„œë§Œ í†µê³¼
        if not doc_matches_category(doc_text, requested_categories):
            continue
        
        # â˜… ì„±ë¶„ í•„í„°ë§: ìš”ì²­í•œ ì„±ë¶„ì´ í¬í•¨ëœ ë¬¸ì„œë§Œ í†µê³¼
        if not doc_matches_ingredients(doc_text, requested_ingredients):
            continue
        
        # â˜… í”¼ë¶€ íƒ€ì… í•„í„°ë§: ìš”ì²­í•œ í”¼ë¶€ íƒ€ì…ì— ì í•©í•œ ë¬¸ì„œë§Œ í†µê³¼
        if not doc_matches_skin_types(doc_text, requested_skin_types):
            continue
        
        hybrid_score = (scores['vector'] * 0.7) + (scores['bm25'] * 0.3)
        
        # ë¦¬ë·°ìˆ˜ ì¶”ì¶œ
        review_count = 0
        match = re.search(r'ë¦¬ë·°ìˆ˜:\s*(\d+)', doc.page_content)
        if match:
            review_count = int(match.group(1))
        
        final_scored.append((review_count, hybrid_score, doc))
    
    # 6. ì •ë ¬: ë¦¬ë·°ìˆ˜ 1ìˆœìœ„, í•˜ì´ë¸Œë¦¬ë“œ ì ìˆ˜ 2ìˆœìœ„
    final_scored.sort(key=lambda x: (x[0], x[1]), reverse=True)
    
    # í•„í„°ë§ ê²°ê³¼ê°€ ì—†ìœ¼ë©´ í•„í„°ë§ ì—†ì´ ì¬ì‹œë„ (Smart Pivot)
    if not final_scored and has_filters:
        for doc_text, scores in doc_scores.items():
            doc = scores['doc']
            hybrid_score = (scores['vector'] * 0.7) + (scores['bm25'] * 0.3)
            review_count = 0
            match = re.search(r'ë¦¬ë·°ìˆ˜:\s*(\d+)', doc.page_content)
            if match:
                review_count = int(match.group(1))
            final_scored.append((review_count, hybrid_score, doc))
        final_scored.sort(key=lambda x: (x[0], x[1]), reverse=True)
    
    return "\n\n".join([d[2].page_content for d in final_scored[:6]])

# ---------------------------------------------------------
# 5. ë§ˆìŠ¤í„° í”„ë¡¬í”„íŠ¸ (ìì—°ìŠ¤ëŸ¬ìš´ ëŒ€í™”ì²´ + Smart Pivot)
# ---------------------------------------------------------
system_prompt = """ë„ˆëŠ” ë‹¤ì´ì†Œì˜ ì „ë¬¸ ë·°í‹° íë ˆì´í„° 'ë·°í‹° ë²„ë””'ì…ë‹ˆë‹¤. 

[ğŸ” í‚¤ì›Œë“œ ë§¤ì¹­ ì›ì¹™ - ìµœìš°ì„ ]
ì§ˆë¬¸ì— í¬í•¨ëœ **í•µì‹¬ ëª…ì‚¬(ì œí’ˆêµ°, ì„±ë¶„, ë¸Œëœë“œëª… ë“±)**ê°€ ëª¨ë‘ ì œí’ˆëª…ì´ë‚˜ ìƒì„¸ì •ë³´ì— í¬í•¨ëœ ì œí’ˆë§Œ ì¶”ì²œí•˜ì‹­ì‹œì˜¤.
- ì˜ˆ: "ì–´ì„±ì´ˆ íŒ¨ë“œ" â†’ "ì–´ì„±ì´ˆ"ì™€ "íŒ¨ë“œ"ê°€ ëª¨ë‘ í¬í•¨ëœ ì œí’ˆë§Œ ì¶”ì²œ
- ì˜ˆ: "ì‹œì¹´ í¬ë¦¼" â†’ "ì‹œì¹´"ì™€ "í¬ë¦¼"ì´ ëª¨ë‘ í¬í•¨ëœ ì œí’ˆë§Œ ì¶”ì²œ
- ì˜ˆ: "ë§ˆë°ì¹´ ì„¸ëŸ¼" â†’ "ë§ˆë°ì¹´"ì™€ "ì„¸ëŸ¼"ì´ ëª¨ë‘ í¬í•¨ëœ ì œí’ˆë§Œ ì¶”ì²œ

[í•„ìˆ˜ ì¤€ìˆ˜ ì‚¬í•­]
1. **ì¹´í…Œê³ ë¦¬ ëª…ì‹œ ì§ˆë¬¸** (ì˜ˆ: "ë¯¸ìŠ¤íŠ¸ ì¶”ì²œí•´ì¤˜")
   - ìš”ì²­í•œ ì œí’ˆêµ°ê³¼ ì¼ì¹˜í•˜ëŠ” ì œí’ˆì„ ìš°ì„  ì¶”ì²œ
   - ì •í™•íˆ ì—†ìœ¼ë©´ ìœ ì‚¬ ì œí’ˆ(ìˆ˜ë¶„í¬ë¦¼ â†’ í¬ë¦¼, ì ¤) ì¶”ì²œ ê°€ëŠ¥

2. **íš¨ëŠ¥ ê¸°ë°˜ ì§ˆë¬¸** (ì˜ˆ: "ì—¬ë“œë¦„ í‰í„° ì—†ì• ê³  ì‹¶ì–´", "í”¼ë¶€ ë’¤ì§‘ì–´ì§„ ê±° ì§„ì •ì‹œí‚¬ ê±°")
   - 'ìƒì„¸ì •ë³´'ì™€ 'í•œì¤„ìš”ì•½'ì—ì„œ ê´€ë ¨ íš¨ëŠ¥ ì°¾ì•„ ì¶”ì²œ
   - ì œí’ˆêµ° ê´€ê³„ì—†ì´ ì¶”ì²œ ê°€ëŠ¥

3. **ë¦¬ë·°ìˆ˜/ì¸ê¸° ê¸°ë°˜ ì§ˆë¬¸** (ì˜ˆ: "ê°€ì¥ ë¦¬ë·° ë§ì€ ì–´ì„±ì´ˆ íŒ¨ë“œ")
   - ë¦¬ë·°ìˆ˜ê°€ ê°€ì¥ ë§ì€ ì œí’ˆì„ ì •í™•íˆ ì°¾ì•„ "ë¦¬ë·°ìˆ˜ê°€ XXê°œë¡œ ê°€ì¥ ë§ìœ¼ë©°" ëª…ì‹œ

4. **ì„±ë³„/ì„ ë¬¼ ê´€ë ¨ ì§ˆë¬¸** (ì˜ˆ: "ë‚¨ì¹œ ì„ ë¬¼ìš©")
   - "ë‚¨ì", "ë‚¨ì„±" í…ìŠ¤íŠ¸ê°€ í¬í•¨ëœ ì œí’ˆ ìš°ì„  ì¶”ì²œ
   - ì—†ìœ¼ë©´ "ë‚¨ì„±ë¶„ë“¤ë„ ì‚¬ìš©í•˜ê¸° ì¢‹ì€" ê°„í¸í•œ ì œí’ˆ ì¶”ì²œ, "ì„ ë¬¼ìš©" í¬ì¸íŠ¸ ì–¸ê¸‰

5. **ê³„ì ˆ ê´€ë ¨ ì§ˆë¬¸** (ì˜ˆ: "ê²¨ìš¸ì— ì“¸ë§Œí•œ ê±°")
   - ê²¨ìš¸ â†’ ë³´ìŠµ, ê±´ì¡° ë°©ì§€ / ì—¬ë¦„ â†’ ì‚°ëœ»í•¨, í”¼ì§€ ì¡°ì ˆ
   - ê³„ì ˆê³¼ ì—°ê´€ëœ íš¨ëŠ¥ ìì—°ìŠ¤ëŸ½ê²Œ ì–¸ê¸‰

6. **ëŒ€ì•ˆ ì œì‹œ (Smart Pivot)**
   - ìš”ì²­ ì œí’ˆì´ ì—†ì–´ë„ "ì—†ë‹¤"ê³  í•˜ì§€ ë§ê³  ë…¼ë¦¬ì  ëŒ€ì•ˆ ì œì‹œ
   - "ë°ì´í„° ë¶€ì¡±", "ì •ë³´ ì—†ìŒ" ê°™ì€ ë¶€ì •ì  í‘œí˜„ ê¸ˆì§€

[ë‹µë³€ êµ¬ì¡°]
ê° ì œí’ˆ ì¶”ì²œ ì‹œ:
1. **ìƒí’ˆëª… ì†Œê°œ**: "ì¶”ì²œë“œë¦¬ëŠ” ì œí’ˆì€ [ìƒí’ˆëª…](URL)ì…ë‹ˆë‹¤."
2. **ìƒì„¸ ì„¤ëª…**: í•µì‹¬ ì„±ë¶„, íš¨ëŠ¥ì„ ìì—°ìŠ¤ëŸ½ê²Œ ì„¤ëª…
3. **ì‚¬ìš©ì ë°˜ì‘**: í¡ìˆ˜ë ¥, ë³´ìŠµë ¥, ìê·¹ë„ ìˆ˜ì¹˜ë¥¼ ë¬¸ì¥ì— ë…¹ì—¬ì„œ ì„¤ëª…
4. **ì‹ ë¢°ë„**: í‰ì ê³¼ ë¦¬ë·°ìˆ˜ ì–¸ê¸‰
   <div class='section-title'>âœ¨ êµ¬ë§¤ì ì¢…í•© ë¦¬ë·° ë¶„ì„</div>
   <div class='summary-box'><span class='summary-label'>ğŸ’¡ [ìƒí’ˆëª…] ë¦¬ë·° ìš”ì•½</span><span class='summary-text'>"[í•œì¤„ìš”ì•½]"</span></div>


**ì¢…í•© ê²°ë¡ ** (2ê°œ ì´ìƒ ì¶”ì²œ ì‹œì—ë§Œ, 1ê°œë§Œ ì¶”ì²œí•œ ê²½ìš° ìƒëµ):
[ë² ìŠ¤íŠ¸ í”½ ì„ ì • ê¸°ì¤€]
1ìˆœìœ„: ì§ˆë¬¸ì˜ í•µì‹¬ ê³ ë¯¼ì— ê°€ì¥ ì í•©í•œ ì œí’ˆ
2ìˆœìœ„: ë¦¬ë·°ìˆ˜ê°€ ë§ê³  í‰ì ì´ ë†’ì€ ì œí’ˆ
3ìˆœìœ„: ì‚¬ìš©ì ë§Œì¡±ë„(í¡ìˆ˜ë ¥, ë³´ìŠµë ¥, ìê·¹ë„)ê°€ ë†’ì€ ì œí’ˆ

<div class='best-box'>
  <div class='best-title'>ğŸ† ë² ìŠ¤íŠ¸ PICK</div>
  <div class='best-text'>[ë² ìŠ¤íŠ¸ ì œí’ˆëª…]ì„ ê°€ì¥ ì¶”ì²œë“œë ¤ìš”! [ì¶”ì²œ ì´ìœ  í•œ ì¤„]</div>
</div>
ì§ˆë¬¸: {question}
ì°¸ì¡° ë¬¸ì„œ: {context}
ë‹µë³€:"""

prompt = PromptTemplate.from_template(system_prompt)
llm = ChatOpenAI(model_name="gpt-4o", temperature=0, streaming=True)

# ---------------------------------------------------------
# 6. UI ë Œë”ë§: ë ˆë“œ í…Œë§ˆ ë°°ë„ˆ ë° ì±—ë´‡ ìºë¦­í„°
# ---------------------------------------------------------
st.markdown("""
    <div class='welcome-banner'>
        <h1>THE DAISO BEAUTY CURATION</h1>
        <p>ë‹¤ì´ì†Œì˜ ë¶‰ì€ ì—´ì •ìœ¼ë¡œ ê³ ê°ë‹˜ì˜ ì•„ë¦„ë‹¤ì›€ì„ ê°€ì¥ ì„¸ë ¨ë˜ê²Œ íë ˆì´íŒ…í•©ë‹ˆë‹¤.</p>
    </div>
    <div class='expert-intro'>
        <div class='expert-avatar'>ğŸ’„</div>
        <div class='expert-text'>
            ë°˜ê°‘ìŠµë‹ˆë‹¤, ê³ ê°ë‹˜. ë‹¤ì´ì†Œ ë·°í‹° íë ˆì´í„°ì…ë‹ˆë‹¤.<br>
            ì˜¤ëŠ˜ì€ ì–´ë–¤ ì„¸ë ¨ëœ ë³€í™”ë¥¼ ê¿ˆê¾¸ê³  ê³„ì‹ ê°€ìš”? í”¼ë¶€ ê³ ë¯¼ì„ ë§ì”€í•´ ì£¼ì‹œë©´ ìµœì ì˜ ì œí’ˆ ë¼ì¸ì—…ì„ ì œì•ˆí•´ ë“œë¦´ê²Œìš”. ğŸ˜Š
        </div>
    </div>
    """, unsafe_allow_html=True)

if "messages" not in st.session_state:
    st.session_state.messages = []

for m in st.session_state.messages:
    with st.chat_message(m["role"]):
        st.markdown(m["content"], unsafe_allow_html=True)

# ---------------------------------------------------------
# 7. ì‹¤í–‰ ë¡œì§: ìŠ¤íŠ¸ë¦¬ë° + ë¬´ì†ì‹¤ ê¸°ëŠ¥
# ---------------------------------------------------------
user_input = st.chat_input("í”¼ë¶€ ê³ ë¯¼ì´ë‚˜ ê¶ê¸ˆí•œ ì œí’ˆì„ ë§ì”€í•´ì£¼ì„¸ìš”!")

if user_input:
    st.session_state.messages.append({"role": "user", "content": user_input})
    with st.chat_message("user"):
        st.markdown(user_input)

    with st.chat_message("assistant"):
        message_placeholder = st.empty()
        full_response = ""
        
        with st.spinner("ë°ì´í„°ì—ì„œ ìµœì ì˜ ëŒ€ì•ˆì„ ë¶„ì„ ì¤‘ì…ë‹ˆë‹¤..."):
            context = get_advanced_context(user_input)
            chain = (
                {"context": lambda x: context, "question": RunnablePassthrough()}
                | prompt | llm | StrOutputParser()
            )
            
            for chunk in chain.stream(user_input):
                full_response += chunk
                time.sleep(0.015)
                message_placeholder.markdown(full_response + "â–Œ", unsafe_allow_html=True)
            
            message_placeholder.markdown(full_response, unsafe_allow_html=True)
            
    st.session_state.messages.append({"role": "assistant", "content": full_response})
