{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "T4",
      "authorship_tag": "ABX9TyPCaUk7P1kKN/Jkr60RsvKZ",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/bineee39/first-repository/blob/master/%EC%9E%91%EC%82%AC%EA%B0%80.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "TQ-R6jVPCoaf",
        "outputId": "514f147f-d545-4691-ba2f-305f0c6638c3"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "2.19.0\n"
          ]
        }
      ],
      "source": [
        "import tensorflow\n",
        "\n",
        "print(tensorflow.__version__)"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## ê°€ì‚¬ëŠ” kaggle ê°€ì‚¬ ë°ì´í„°ë¥¼ ê°€ì ¸ì™”ìŠµë‹ˆë‹¤.\n"
      ],
      "metadata": {
        "id": "CDOyEUjBjtsv"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import os\n",
        "import re\n",
        "import zipfile\n",
        "import glob\n",
        "import numpy as np\n",
        "import tensorflow as tf\n",
        "from sklearn.model_selection import train_test_split\n",
        "import pandas as pd\n",
        "\n",
        "# 1. íŒŒì¼ ê²½ë¡œ ì„¤ì • (ì½”ë© ë¡œì»¬ íŒŒì¼ ì‹œìŠ¤í…œ ê¸°ì¤€)\n",
        "# í˜„ì¬ ì´ë¯¸ì§€ë¥¼ ë³´ë©´ íŒŒì¼ì´ ë£¨íŠ¸ ê²½ë¡œì— ìœ„ì¹˜í•œ ê²ƒìœ¼ë¡œ ë³´ì…ë‹ˆë‹¤.\n",
        "zip_path = './archive (7).zip'\n",
        "\n",
        "# ì••ì¶• í•´ì œí•  ì„ì‹œ ë””ë ‰í† ë¦¬ ì„¤ì •\n",
        "extract_dir = './extracted_lyrics'\n",
        "os.makedirs(extract_dir, exist_ok=True)\n",
        "print(f\" ì„¤ì • ì™„ë£Œ. ì••ì¶• íŒŒì¼ ê²½ë¡œ: {zip_path}\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "tS5ytgJU6a6j",
        "outputId": "98e03c2e-571a-4d1f-897f-4bd28e6d87ca"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            " ì„¤ì • ì™„ë£Œ. ì••ì¶• íŒŒì¼ ê²½ë¡œ: ./archive (7).zip\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# 2. ZIP íŒŒì¼ ì••ì¶• í•´ì œ\n",
        "try:\n",
        "    with zipfile.ZipFile(zip_path, 'r') as zip_ref:\n",
        "        print(f\"ğŸ” {zip_path} ì••ì¶• í•´ì œ ì¤‘...\")\n",
        "        zip_ref.extractall(extract_dir)\n",
        "    print(\"ì••ì¶• í•´ì œ ì™„ë£Œ!\")\n",
        "except FileNotFoundError:\n",
        "    print(f\"ì˜¤ë¥˜: ì§€ì •ëœ ê²½ë¡œì— íŒŒì¼ì´ ì—†ìŠµë‹ˆë‹¤. ê²½ë¡œë¥¼ í™•ì¸í•´ ì£¼ì„¸ìš”: {zip_path}\")\n",
        "    raw_corpus = []\n",
        "else:\n",
        "    # 3. ì••ì¶• í•´ì œëœ ë””ë ‰í† ë¦¬ì—ì„œ ëª¨ë“  CSV íŒŒì¼ ì½ê¸°\n",
        "    csv_files = glob.glob(os.path.join(extract_dir, '**', '*.csv'), recursive=True)\n",
        "    raw_corpus = []\n",
        "\n",
        "    print(f\"{len(csv_files)}ê°œì˜ CSV íŒŒì¼ì—ì„œ ë°ì´í„° ì½ê¸° ì‹œì‘...\")\n",
        "\n",
        "    TEXT_COLUMN_NAME = 'Lyric'\n",
        "\n",
        "    for csv_file in csv_files:\n",
        "        try:\n",
        "            df = pd.read_csv(csv_file)\n",
        "\n",
        "            # ê°€ì‚¬ í…ìŠ¤íŠ¸ ì»¬ëŸ¼ì´ ë°ì´í„°í”„ë ˆì„ì— ìˆëŠ”ì§€ í™•ì¸\n",
        "            if TEXT_COLUMN_NAME not in df.columns:\n",
        "                 print(f\"ê²½ê³ : {csv_file}ì— '{TEXT_COLUMN_NAME}' ì»¬ëŸ¼ì´ ì—†ìŠµë‹ˆë‹¤. ì²« ë²ˆì§¸ ì»¬ëŸ¼ì„ ì‚¬ìš©í•©ë‹ˆë‹¤.\")\n",
        "                 text_column = df.columns[0]\n",
        "            else:\n",
        "                 text_column = TEXT_COLUMN_NAME\n",
        "\n",
        "            # í…ìŠ¤íŠ¸ê°€ NaNì´ ì•„ë‹Œ í–‰ë§Œ í•„í„°ë§í•˜ì—¬ ë¦¬ìŠ¤íŠ¸ì— ì¶”ê°€\n",
        "            lines = df[text_column].dropna().astype(str).tolist()\n",
        "            raw_corpus.extend(lines)\n",
        "\n",
        "        except Exception as e:\n",
        "            print(f\"ê²½ê³ : {csv_file} ì²˜ë¦¬ ì¤‘ ì˜¤ë¥˜ ë°œìƒ - {e}\")\n",
        "\n",
        "    # ëª¨ë“  ë¦¬ìŠ¤íŠ¸ ìš”ì†Œê°€ ë¬¸ì¥ ë‹¨ìœ„ë¡œ ì €ì¥ë˜ë„ë¡ ë¶„ë¦¬ ë° ë¹ˆ ì¤„ ì œê±°\n",
        "    final_corpus = []\n",
        "    for line in raw_corpus:\n",
        "        final_corpus.extend(line.split('\\n'))\n",
        "\n",
        "    raw_corpus = [line.strip() for line in final_corpus if line.strip()]\n",
        "\n",
        "    print(f\"ë°ì´í„° í¬ê¸°: {len(raw_corpus)} ë¬¸ì¥\")\n",
        "    print(\"Examples:\\n\", raw_corpus[:3])"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ragzLDr0-5zj",
        "outputId": "4a6d7318-bb91-430c-d378-9e68c8ebaacb"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "ğŸ” ./archive (7).zip ì••ì¶• í•´ì œ ì¤‘...\n",
            "ì••ì¶• í•´ì œ ì™„ë£Œ!\n",
            "21ê°œì˜ CSV íŒŒì¼ì—ì„œ ë°ì´í„° ì½ê¸° ì‹œì‘...\n",
            "ë°ì´í„° í¬ê¸°: 5981 ë¬¸ì¥\n",
            "Examples:\n",
            " [\"rihanna work work work work work work he said me haffi work work work work work work he see me do mi dirt dirt dirt dirt dirt dirt so me put in work work work work work work when you ah guh learn learn learn learn learn meh nuh cyar if him hurt hurt hurt hurt hurting   rihanna dry me a desert him nuh time to have you lurking him ah go act like he nuh like it you know i dealt with you the nicest nuh body touch me you nuh righteous nuh badda text me in a crisis i believed all of your dreams adoration you took my heart and my keys and my patience you took my heart on my sleeve for decoration you mistaken my love i brought for you for foundation all that i wanted from you was to give me something that i never had something that you've never seen something that you've never been mmmmm but i wake up and act like nothing's wrong just get ready fi   rihanna work work work work work work he said me haffi work work work work work work he see me do mi dirt dirt dirt dirt dirt dirt so me put in work work work work work work ner ner ner ner ner ner when yuh ago learn learn learn learn learn learn before the tables turn turn turn turn turn turn   rihanna beg you something please baby don't you leave dont leave me stuck here in the streets uh huh if i get another chance to i will never no never neglect you i mean who am i to hold your past against you i just hope that it gets to you i hope that you see this through i hope that you see this true what can i say please recognize i'm tryin' babe i have to   rihanna work work work work work work he said me haffi work work work work work work he see me do mi dirt dirt dirt dirt dirt dirt so me put in work work work work work work when you ah guh learn learn learn learn learn meh nuh cyar if him hurt hurt hurt hurt hurting   drake with rihanna yeah okay you need to get done done done done at work come over we just need to slow the motion don't give that away to no one long distance i need you when i see potential i just gotta see it through if you had a twin i would still choose you i don't wanna rush into it if it's too soon but i know you need to get done done done done if you come over sorry if i'm way less friendly i got niggas tryna end me oh i spilled all my emotions tonight im sorry rollin' rollin' rollin' rollin' rollin' how many more shots until you're rollin' we just need a face to face you could pick the time and the place you spent some time away now you need to forward and give me all the   rihanna  drake work work work work work work he said me haffi work work work work work work he se me do mi dirt dirt dirt dirt dirt dirt so me put in work work work work work work when you ah guh learn learn learn learn learn meh nuh cyar if him hurt hurt hurt hurt hurting   rihanna mmmmm mmmmm mmmmm mmmmm work work work work work work mmmmm mmmmm  click here to learn more about the making of work  produced by boida\", \"and you got me like oh what you want from me what you want from me and i tried to buy your pretty heart but the price too high baby you got me like oh you love when i fall apart fall apart so you can put me together and throw me against the wall  pre baby you got me like iiiiiiiiihiii woo iiiiiiiiihiii don't you stop loving me loving me don't quit loving me loving me just start loving me loving me owwwwww   and babe im fistfighting with fire just to get close to you can we burn something babe and i run for miles just to get a taste must be love on the brain thats got me feeling this way it beats me black and blue but it fucks me so good and i cant get enough must be love on the brain yeah and it keeps cursing my name no matter what i do im no good without you and i cant get enough must be love on the brain   baby keep loving me just love me yeah just love me all you need to do is love me yeah got me like ahhahaowww im tired of being played like a violin what do i gotta do to get in your motherfuckin heart baby like iiiiiiiiihiii woo iiiiiiiiihiii  pre baby you got me like iiiiiiiiihiii woo iiiiiiiiihiii don't you stop loving me loving me don't quit loving me loving me just start loving me loving me ooooooohhh   and babe im fistfighting with fire just to get close to you can we burn something babe and i run for miles just to get a taste must be love on the brain thats got me feeling this way it beats me black and blue but it fucks me so good and i cant get enough must be love on the brain yeah and it keeps cursing my name no matter what i do im no good without you and i cant get enough must be love on the brain  click here to learn more about the making of love on the brain\", \"yg mustard on the beat ho   i was good on my own that's the way it was that's the way it was you was good on the low for a faded fuck on some faded love shit what the fuck you complaining for feeling jaded huh used to trip off that shit i was kickin' to you had some fun on the run though i'll give it to you  pre but baby don't get it twisted you was just another nigga on the hit list tryna fix your inner issues with a bad bitch didn't they tell you that i was a savage fuck ya white horse and ya carriage bet you never could imagine never told you you could have it   you needed me ooh you needed me to feel a little more and give a little less know you hate to confess but baby ooh you needed me   you been rollin' around shit i'm rollin' up light and roll it up break it down like a pound shit was never us shit was never us that's the real on the real are you serious how you feel how you feel used to trip off that shit i was kickin' to ya had some fun on the run though i'll give it to ya  pre but baby don't get it twisted you was just another nigga on the hit list tryna fix your inner issues with a bad bitch didn't they tell you that i was a savage fuck ya white horse and ya carriage bet you never could imagine never told you you could have it   you needed me ooh you needed me to feel a little more and give a little less know you hate to confess but baby ooh you needed me\"]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# 4. ë¬¸ì¥ ì •ì œ í•¨ìˆ˜\n",
        "def preprocess_sentence(sentence):\n",
        "    sentence = sentence.lower().strip()\n",
        "    sentence = re.sub(r\"([?.!,Â¿])\", r\" \\1 \", sentence)\n",
        "    sentence = re.sub(r'[\" \"]+', \" \", sentence)\n",
        "    sentence = re.sub(r\"[^a-zA-Z?.!,Â¿]+\", \" \", sentence)\n",
        "    sentence = sentence.strip()\n",
        "    sentence = '<start> ' + sentence + ' <end>'\n",
        "    return sentence\n",
        "\n",
        "# ì •ì œ í•¨ìˆ˜ ì ìš©\n",
        "corpus = list(map(preprocess_sentence, raw_corpus))\n",
        "print(f\"ì •ì œëœ ë°ì´í„° í¬ê¸°: {len(corpus)}\")\n",
        "\n",
        "# 5. í† í°í™” ë° íŒ¨ë”© í•¨ìˆ˜\n",
        "def tokenize(corpus, num_words, maxlen):\n",
        "    tokenizer = tf.keras.preprocessing.text.Tokenizer(\n",
        "        num_words=num_words,\n",
        "        filters=' ',\n",
        "        oov_token=\"<unk>\"\n",
        "    )\n",
        "    tokenizer.fit_on_texts(corpus)\n",
        "    sequences = tokenizer.texts_to_sequences(corpus)\n",
        "\n",
        "    # Filter out sequences longer than maxlen\n",
        "    filtered_sequences = [seq for seq in sequences if len(seq) <= maxlen]\n",
        "\n",
        "    tensor = tf.keras.preprocessing.sequence.pad_sequences(filtered_sequences, padding='post', maxlen=maxlen)\n",
        "    return tensor, tokenizer, filtered_sequences # Return filtered_sequences as well\n",
        "\n",
        "# í•˜ì´í¼íŒŒë¼ë¯¸í„° ì„¤ì •\n",
        "NUM_WORDS = 12000 # ì‚¬ìš©í•  ë‹¨ì–´ ê°œìˆ˜\n",
        "MAX_LEN = 15      # ë¬¸ì¥ ìµœëŒ€ ê¸¸ì´\n",
        "\n",
        "tensor, tokenizer, filtered_sequences = tokenize(corpus, NUM_WORDS, MAX_LEN)\n",
        "print(f\"í† í°í™” í›„ ê¸¸ì´ {MAX_LEN} ì´í•˜ì¸ ë¬¸ì¥ ìˆ˜: {len(filtered_sequences)} ë¬¸ì¥ (í•„í„°ë§ ì „: {len(corpus)} ë¬¸ì¥)\")\n",
        "print(f\"í…ì„œ ë³€í™˜ ì™„ë£Œ. í…ì„œ ëª¨ì–‘: {tensor.shape}\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "xRznLLCs_eVj",
        "outputId": "76aa898b-4549-4519-b795-f882ef11d8d7"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "ì •ì œëœ ë°ì´í„° í¬ê¸°: 5981\n",
            "í† í°í™” í›„ ê¸¸ì´ 15 ì´í•˜ì¸ ë¬¸ì¥ ìˆ˜: 114 ë¬¸ì¥ (í•„í„°ë§ ì „: 5981 ë¬¸ì¥)\n",
            "í…ì„œ ë³€í™˜ ì™„ë£Œ. í…ì„œ ëª¨ì–‘: (114, 15)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# 6. Source (ì…ë ¥)ì™€ Target (ì •ë‹µ) ë¶„ë¦¬\n",
        "enc_inputs = tensor[:, :-1]\n",
        "dec_targets = tensor[:, 1:]\n",
        "\n",
        "# 7. í›ˆë ¨ ë°ì´í„°ì™€ ê²€ì¦ ë°ì´í„° ë¶„ë¦¬ (80% í›ˆë ¨, 20% ê²€ì¦)\n",
        "enc_train, enc_val, dec_train, dec_val = train_test_split(\n",
        "    enc_inputs,\n",
        "    dec_targets,\n",
        "    test_size=0.2,\n",
        "    random_state=42,\n",
        "    shuffle=True\n",
        ")\n",
        "\n",
        "# 8. tf.data.Dataset êµ¬ì„±\n",
        "BUFFER_SIZE = len(enc_train)\n",
        "BATCH_SIZE = 64 # ì½”ë© ë©”ëª¨ë¦¬ë¥¼ ê³ ë ¤í•œ ë°°ì¹˜ ì‚¬ì´ì¦ˆ\n",
        "\n",
        "train_dataset = tf.data.Dataset.from_tensor_slices((enc_train, dec_train))\n",
        "train_dataset = train_dataset.shuffle(BUFFER_SIZE).batch(BATCH_SIZE, drop_remainder=True).repeat()\n",
        "\n",
        "val_dataset = tf.data.Dataset.from_tensor_slices((enc_val, dec_val))\n",
        "val_dataset = val_dataset.shuffle(BUFFER_SIZE).batch(BATCH_SIZE, drop_remainder=False)\n",
        "\n",
        "print(\"\\në°ì´í„°ì…‹ ì¤€ë¹„ ì™„ë£Œ.\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "kKDfFnVk_kVt",
        "outputId": "5c1e82c5-396f-45e4-98b6-9a2691ef3599"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "ë°ì´í„°ì…‹ ì¤€ë¹„ ì™„ë£Œ.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import tensorflow as tf\n",
        "\n",
        "# 9. ëª¨ë¸ ì •ì˜\n",
        "class TextGenerator(tf.keras.Model):\n",
        "    def __init__(self, vocab_size, embedding_size, hidden_size):\n",
        "        super().__init__()\n",
        "        self.embedding = tf.keras.layers.Embedding(vocab_size, embedding_size)\n",
        "        self.rnn_1 = tf.keras.layers.LSTM(hidden_size, return_sequences=True)\n",
        "        self.rnn_2 = tf.keras.layers.LSTM(hidden_size, return_sequences=True)\n",
        "        self.linear = tf.keras.layers.Dense(vocab_size)\n",
        "\n",
        "    def call(self, x):\n",
        "        out = self.embedding(x)\n",
        "        out = self.rnn_1(out)\n",
        "        out = self.rnn_2(out)\n",
        "        out = self.linear(out)\n",
        "        return out\n",
        "\n",
        "# í•˜ì´í¼íŒŒë¼ë¯¸í„° ì„¤ì •\n",
        "VOCAB_SIZE = tokenizer.num_words + 1\n",
        "EMBEDDING_SIZE = 256\n",
        "HIDDEN_SIZE = 256\n",
        "\n",
        "model = TextGenerator(VOCAB_SIZE, EMBEDDING_SIZE, HIDDEN_SIZE)\n",
        "\n",
        "# 10. ëª¨ë¸ ì»´íŒŒì¼ ë° í•™ìŠµ\n",
        "optimizer = tf.keras.optimizers.Adam(learning_rate=0.001)\n",
        "loss = tf.keras.losses.SparseCategoricalCrossentropy(\n",
        "    from_logits=True,\n",
        "    reduction='none'\n",
        ")\n",
        "\n",
        "model.compile(loss=loss, optimizer=optimizer)\n",
        "\n",
        "# Calculate steps per epoch based on estimated time (approx 5 minutes per epoch)\n",
        "# Original: 27727 steps took 1022s -> ~36.8ms/step\n",
        "# For 300s (5 minutes) / 0.0368s/step = ~8152 steps. Let's use 8139 for safety margin.\n",
        "TRAIN_STEPS_PER_EPOCH = 8139\n",
        "\n",
        "# Calculate validation steps (validation data size: len(enc_val) = 23, BATCH_SIZE = 64)\n",
        "# ceil(23 / 64) = 1\n",
        "VALIDATION_STEPS = 1\n",
        "\n",
        "print(f\"\\nëª¨ë¸ í•™ìŠµ ì‹œì‘. (Epochs=5, steps_per_epoch={TRAIN_STEPS_PER_EPOCH})\")\n",
        "history = model.fit(\n",
        "    train_dataset,\n",
        "    validation_data=val_dataset,\n",
        "    epochs=5,\n",
        "    steps_per_epoch=TRAIN_STEPS_PER_EPOCH,\n",
        "    validation_steps=VALIDATION_STEPS\n",
        ")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "IGrE0XOo_pjj",
        "outputId": "af28583e-8ca8-4f0f-8880-4df6252e520f"
      },
      "execution_count": 29,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "ëª¨ë¸ í•™ìŠµ ì‹œì‘. (Epochs=5, steps_per_epoch=8139)\n",
            "Epoch 1/5\n",
            "\u001b[1m8139/8139\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m301s\u001b[0m 37ms/step - loss: 1.0982 - val_loss: 5.6645\n",
            "Epoch 2/5\n",
            "\u001b[1m8139/8139\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m299s\u001b[0m 37ms/step - loss: 0.2909 - val_loss: 7.0445\n",
            "Epoch 3/5\n",
            "\u001b[1m8139/8139\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m299s\u001b[0m 37ms/step - loss: 0.2904 - val_loss: 7.4892\n",
            "Epoch 4/5\n",
            "\u001b[1m8139/8139\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m302s\u001b[0m 37ms/step - loss: 0.2902 - val_loss: 8.0588\n",
            "Epoch 5/5\n",
            "\u001b[1m8139/8139\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m304s\u001b[0m 37ms/step - loss: 0.2901 - val_loss: 8.3346\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import tensorflow as tf\n",
        "import numpy as np # tf.random.categorical ì‚¬ìš©ì„ ìœ„í•´ numpy í•„ìš”\n",
        "\n",
        "# 11. ë¬¸ì¥ ìƒì„± í•¨ìˆ˜ (Sampling ì ìš©)\n",
        "def generate_text_sampling(model, tokenizer, init_sentence=\"<start>\", max_len=60, temperature=1.0):\n",
        "\n",
        "    # [ì…ë ¥ ë¬¸ì¥ ì „ì²˜ë¦¬ ë¶€ë¶„ì€ ì´ì „ê³¼ ë™ì¼]\n",
        "    if not init_sentence.startswith('<start>'):\n",
        "        if ' <end>' in init_sentence:\n",
        "            init_sentence = init_sentence.split(' <end>')[0]\n",
        "        if not init_sentence.startswith('<start>'):\n",
        "            init_sentence = '<start> ' + init_sentence\n",
        "\n",
        "    test_input = tokenizer.texts_to_sequences([init_sentence])\n",
        "    test_tensor = tf.convert_to_tensor(test_input, dtype=tf.int64)\n",
        "    end_token = tokenizer.word_index.get(\"<end>\")\n",
        "\n",
        "    while True:\n",
        "        predict = model(test_tensor)\n",
        "\n",
        "        # 1. Temperature ì ìš© (Softmax ì „ì— í™•ë¥  ë¶„í¬ì˜ ê²½ì‚¬ë¥¼ ì™„ë§Œí•˜ê²Œ ë§Œë“¦)\n",
        "        predict = predict / temperature\n",
        "\n",
        "        # 2. í™•ë¥  ë¶„í¬ ê³„ì‚° (Softmax)\n",
        "        predict_proba = tf.nn.softmax(predict, axis=-1)[:, -1]\n",
        "\n",
        "        # 3. í™•ë¥  ë¶„í¬ì— ë”°ë¼ ë‹¨ì–´ ìƒ˜í”Œë§ (ê°€ì¥ ë†’ì€ í™•ë¥ ë§Œ ì„ íƒí•˜ì§€ ì•ŠìŒ)\n",
        "        # tf.math.logë¥¼ ì‚¬ìš©í•˜ì—¬ logitì„ í™•ë¥ ë¡œ ë³€í™˜\n",
        "        predict_word = tf.random.categorical(\n",
        "            tf.math.log(predict_proba), num_samples=1\n",
        "        )\n",
        "        predict_word = tf.squeeze(predict_word, axis=1) # [1, 1] -> [1] í…ì„œ ëª¨ì–‘ ì •ë¦¬\n",
        "\n",
        "        # 4. test_tensor ì—…ë°ì´íŠ¸ (ìˆœí™˜)\n",
        "        test_tensor = tf.concat([test_tensor, tf.expand_dims(predict_word, axis=0)], axis=-1)\n",
        "\n",
        "        # ì¢…ë£Œ ì¡°ê±´\n",
        "        if end_token and predict_word.numpy()[0] == end_token: break\n",
        "        if test_tensor.shape[1] >= max_len: break\n",
        "\n",
        "    generated = \"\"\n",
        "    for word_index in test_tensor[0].numpy():\n",
        "        word = tokenizer.index_word.get(word_index, \"\")\n",
        "        if word not in ('<start>', '<end>', '<pad>', '<unk>', ''):\n",
        "            generated += word + \" \"\n",
        "\n",
        "    return generated.strip()\n",
        "\n",
        "# 12. ê²°ê³¼ í™•ì¸ (Sampling ì ìš©)\n",
        "# temperature=1.0ì€ ì¼ë°˜ì ì¸ ë¬´ì‘ìœ„ì„±, 0.5ëŠ” ë” ë³´ìˆ˜ì , 1.5ëŠ” ë” ì°½ì˜ì /ë¬´ì‘ìœ„ì \n",
        "TEMPERATURE = 0.8\n",
        "\n",
        "print(f\"\\nğŸ”¥ ëª¨ë¸ì´ ìƒì„±í•œ ê°€ì‚¬ (Sampling ì ìš©, Temperature={TEMPERATURE}):\\n\")\n",
        "\n",
        "print(\"--- i wishë¡œ ì‹œì‘ ---\")\n",
        "print(generate_text_sampling(model, tokenizer, init_sentence=\"i wish\", max_len=60, temperature=TEMPERATURE))\n",
        "print(generate_text_sampling(model, tokenizer, init_sentence=\"i wish\", max_len=60, temperature=TEMPERATURE))\n",
        "print(generate_text_sampling(model, tokenizer, init_sentence=\"i wish\", max_len=60, temperature=TEMPERATURE))\n",
        "\n",
        "print(\"\\n--- i see theë¡œ ì‹œì‘ ---\")\n",
        "print(generate_text_sampling(model, tokenizer, init_sentence=\"i see the\", max_len=60, temperature=TEMPERATURE))\n",
        "print(generate_text_sampling(model, tokenizer, init_sentence=\"i see the\", max_len=60, temperature=TEMPERATURE))\n",
        "print(generate_text_sampling(model, tokenizer, init_sentence=\"i see the\", max_len=60, temperature=TEMPERATURE))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "iwMKK6C0kJkj",
        "outputId": "5246b98f-b438-41e2-be9b-e2a0c76dc901"
      },
      "execution_count": 36,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "ğŸ”¥ ëª¨ë¸ì´ ìƒì„±í•œ ê°€ì‚¬ (Sampling ì ìš©, Temperature=0.8):\n",
            "\n",
            "--- i wishë¡œ ì‹œì‘ ---\n",
            "i wish can you\n",
            "i wish can you\n",
            "i wish can you\n",
            "\n",
            "--- i see theë¡œ ì‹œì‘ ---\n",
            "i see the full\n",
            "i see the full\n",
            "i see the full\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        ".......kaggle ê°€ì‚¬ì— ì¹´ë””ë¹„ê°€ ìˆì—ˆëŠ”ë° ê°€ì‚¬ê°€ ë„ˆë¬´ ì„ ì •ì ì´ë¼ ê·¸ëŸ°ì§€ ì²˜ìŒì— ê°€ì‚¬ ë½‘ì•˜ì„ë•Œ ì´ìƒí•œ ê°€ì‚¬ê°€ ë‚˜ì™€ì„œ ì½”ë“œìˆ˜ì •í–ˆìŠµë‹ˆë‹¤.................."
      ],
      "metadata": {
        "id": "8z7rwTx2lktt"
      }
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "jZGdawKNkTAd"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}